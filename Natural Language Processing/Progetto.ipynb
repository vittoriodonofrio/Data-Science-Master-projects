{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Punto 1**"
      ],
      "metadata": {
        "id": "eufUQGi7tkFE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lXj-oSwSJy5Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, make_scorer, confusion_matrix\n",
        "import string\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_85HnCOLJSm",
        "outputId": "3879d750-cc84-4ee8-fa99-a3b282991dea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/spam_dataset.csv', usecols =['text', 'label_num'])"
      ],
      "metadata": {
        "id": "v73GGtCiLNCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7yjwCalaNTCI",
        "outputId": "16741e95-8472-48f2-d254-5a3397207097"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label_num\n",
              "0  Subject: enron methanol ; meter # : 988291\\nth...          0\n",
              "1  Subject: hpl nom for january 9 , 2001\\n( see a...          0\n",
              "2  Subject: neon retreat\\nho ho ho , we ' re arou...          0\n",
              "3  Subject: photoshop , windows , office . cheap ...          1\n",
              "4  Subject: re : indian springs\\nthis deal is to ...          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a44b87a-6bab-45d1-979f-35d5b9482691\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Subject: enron methanol ; meter # : 988291\\nth...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Subject: hpl nom for january 9 , 2001\\n( see a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Subject: neon retreat\\nho ho ho , we ' re arou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Subject: re : indian springs\\nthis deal is to ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a44b87a-6bab-45d1-979f-35d5b9482691')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8a44b87a-6bab-45d1-979f-35d5b9482691 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8a44b87a-6bab-45d1-979f-35d5b9482691');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e7630a2f-30e2-44e1-8472-dbcd32f3a306\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e7630a2f-30e2-44e1-8472-dbcd32f3a306')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e7630a2f-30e2-44e1-8472-dbcd32f3a306 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5171,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4993,\n        \"samples\": [\n          \"Subject: hpl / conoco - teco waha 03 / 23 / 01 purchase\\ndaren , conoco invoiced hpl at $ 5 . 87 for 03 / 23 at pgev / waha and deal ticket 685350 shows $ 4 . 87 . can you confirm the price ? thanks .\",\n          \"Subject: holiday on - call data\\npipeline contact phone fax pager\\nblack marlin blair lichentwalter 713 853 - 7367 713 646 - 3201 ( h )\\n281 370 - 1866\\ndebbie thompson 713 853 - 3144 713 646 - 3201\\n( noms due today for 23 rd through 27 th )\\nchannel jim tobacco 713 420 - 2159\\ngas control 1 505 599 - 2333\\n( open thursday . noms will be due through monday )\\ncentana william spekels 713 627 - 6290 713 762 - 3450\\ndonna spencer 713 627 - 6255\\ngas control 1 888 204 - 1718\\n( noms due today for 23 rd through 27 th )\\nduke energy annette anderson 713 260 - 8603 713 949 - 3026\\n( on call ) bob moseman 713 - 260 - 8698 ( thursday )\\nopen tomorrow - noms will be due thru the 27 th )\\nlonestar gary gafford 214 670 - 2674 214 875 - 3810\\ngas control 214 875 - 2455 or 2456\\n( noms due today , 23 rd thru 27 th )\\nnorthern natural ben markey 853 - 7581 cell 713 446 - 9404 800 931 - 0398\\n( on call ) charlie mosey 853 - 1520\\ngas control 853 -\\n( open thursday - noms due thru 27 th . )\\neast trans - east texas\\ntejas gas control 713 767 - 5366\\npaula svehla 713 230 - 3569\\nmickey chapman 713 230 - 3546\\n( open thursday - noms due thru 27 th )\\nmidcon ( y 2 k ) ken nachlinger 713 369 - 9284 713 369 - 9375 888 733 - 5954\\n( on call ) steven 888 790 - 0255\\n( y 2 k ) don 888 733 - 4602\\ngas control 713 369 - 9200\\n( noms due today , 23 rd thru 27 th )\\nmoss bluff no current business\",\n          \"Subject: gas day 2 / 08 / 01\\nwe agree :\\nteco tap nom = 40 . 000 ; actual 41 . 358\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by melissa jones / texas utilities on\\n02 / 09 / 2001\\n10 : 15 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\nkponton @ duke - energy . com on 02 / 09 / 2001 09 : 15 : 35 am\\nto : david avila / lsp / enserch / us @ tu , charlie stone / texas utilities @ tu , melissa\\njones / texas utilities @ tu\\ncc :\\nsubject : gas day 2 / 08 / 01\\nnom = 40 , 000 mmbtu ' s\\nactual flow = 39 , 959 mcf , 41 , 358 mmbtu ' s\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_num\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC2MTMzBNVTN",
        "outputId": "008625ca-195d-4c15-ca48-c66ba8160085"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5171, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYvtdaSlSU6Z",
        "outputId": "bfbf1f67-a5e5-4adf-c540-aef27117cc6a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text         0\n",
              "label_num    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non ci sono valori mancanti nel dataframe."
      ],
      "metadata": {
        "id": "IZZOm3LA6yOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['label_num'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PERGhoEOTuT",
        "outputId": "886ded4a-c4f3-48da-9edf-9f25f3dea379"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    3672\n",
              "1    1499\n",
              "Name: label_num, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il dataset è sbilanciato in favore della classe 0, quella delle mail non spam. In questo caso, per valutare il modello dobbiamo quindi controllare non solo la accuracy ma anche la recall perchè può essere che il modello tenda a classificare ogni osservazione come appartenente alla classe 0 risultando in una accuracy comunque alta. Guardando anche la recall, si controlla il numero di falsi negativi."
      ],
      "metadata": {
        "id": "ih9kJ0JlVLbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing del testo**"
      ],
      "metadata": {
        "id": "-M5_4OkLfswJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardando la colonna text del dataframe si intuisce che è presente spesso la parola \"Subject\", ovvero \"oggetto\" della mail. L'output della cella seguente indica che questa parola è presente in tutte le osservazioni della colonna text, non essendo informativa, si può omettere in fase di pulizia del testo."
      ],
      "metadata": {
        "id": "hSbHPvTuhSA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'].str.contains('Subject').all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYRLUWJ3gkmU",
        "outputId": "94d249c5-eee3-4d66-dd9c-9820982b95fe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_stopwords = stopwords.words('english')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "\n",
        "def data_cleaner(dataset):\n",
        "    dataset_to_return = []\n",
        "    for sentence in dataset:\n",
        "        sentence = sentence.lower()\n",
        "        sentence = sentence.replace(\"Subject\", \" \")\n",
        "        for c in string.punctuation:\n",
        "            sentence = sentence.replace(c, \" \")\n",
        "        document = nlp(sentence)\n",
        "        sentence = ' '.join(token.lemma_ for token in document)\n",
        "        sentence = ' '.join(word for word in sentence.split() if word not in english_stopwords)\n",
        "        sentence = re.sub('\\d', '', sentence)\n",
        "        dataset_to_return.append(sentence)\n",
        "\n",
        "    return dataset_to_return"
      ],
      "metadata": {
        "id": "n4EosdZEVJxS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_cleaned = data_cleaner(df['text'])"
      ],
      "metadata": {
        "id": "-meGoVJehH-D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Costruzione del modello**"
      ],
      "metadata": {
        "id": "ZXQzm5O8uxig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bow_tfidf(dataset, tfidf_vectorizer):\n",
        "    if tfidf_vectorizer == None:\n",
        "        tfidf_vectorizer = TfidfVectorizer()\n",
        "        X = tfidf_vectorizer.fit_transform(dataset)\n",
        "    else:\n",
        "        X = tfidf_vectorizer.transform(dataset)\n",
        "\n",
        "    return X.toarray(), tfidf_vectorizer"
      ],
      "metadata": {
        "id": "IuiuOdUgiBh8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = df['label_num'].values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(text_cleaned, Y, test_size = 0.2, random_state = 43)\n",
        "\n",
        "X_train, tfidf_fitted = bow_tfidf(X_train, None)\n",
        "X_test, _ = bow_tfidf(X_test, tfidf_fitted)"
      ],
      "metadata": {
        "id": "bgFaq53YiIz5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "clf = MLPClassifier(activation='logistic',\n",
        "                    hidden_layer_sizes=(100,),\n",
        "                    max_iter=100,\n",
        "                    solver='adam',\n",
        "                    tol=0.005,\n",
        "                    verbose=True)\n",
        "\n",
        "clf.fit(X_train, Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "id": "i1kvc5WJqI8h",
        "outputId": "f1a6be66-fb59-4c60-977a-67b532856847"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.69312540\n",
            "Iteration 2, loss = 0.56392712\n",
            "Iteration 3, loss = 0.53150745\n",
            "Iteration 4, loss = 0.50053331\n",
            "Iteration 5, loss = 0.46735239\n",
            "Iteration 6, loss = 0.43086553\n",
            "Iteration 7, loss = 0.39105636\n",
            "Iteration 8, loss = 0.34905743\n",
            "Iteration 9, loss = 0.30727910\n",
            "Iteration 10, loss = 0.26752570\n",
            "Iteration 11, loss = 0.23128725\n",
            "Iteration 12, loss = 0.19995000\n",
            "Iteration 13, loss = 0.17307296\n",
            "Iteration 14, loss = 0.15070748\n",
            "Iteration 15, loss = 0.13198120\n",
            "Iteration 16, loss = 0.11639859\n",
            "Iteration 17, loss = 0.10341611\n",
            "Iteration 18, loss = 0.09249709\n",
            "Iteration 19, loss = 0.08321932\n",
            "Iteration 20, loss = 0.07539059\n",
            "Iteration 21, loss = 0.06865623\n",
            "Iteration 22, loss = 0.06288217\n",
            "Iteration 23, loss = 0.05787118\n",
            "Iteration 24, loss = 0.05352343\n",
            "Iteration 25, loss = 0.04973931\n",
            "Iteration 26, loss = 0.04637356\n",
            "Iteration 27, loss = 0.04340946\n",
            "Iteration 28, loss = 0.04079467\n",
            "Iteration 29, loss = 0.03845832\n",
            "Iteration 30, loss = 0.03635298\n",
            "Iteration 31, loss = 0.03449943\n",
            "Iteration 32, loss = 0.03278939\n",
            "Iteration 33, loss = 0.03125525\n",
            "Iteration 34, loss = 0.02985464\n",
            "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='logistic', max_iter=100, tol=0.005, verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, max_iter=100, tol=0.005, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, max_iter=100, tol=0.005, verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(y_true, y_pred):\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred):.3f}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.3f}\")\n",
        "    print(f\"Precision: {precision_score(y_true, y_pred):.3f}\")\n",
        "    print(f\"f1 score: {f1_score(y_true, y_pred):.3f}\")\n",
        "    print(f\"MATRICE DI CONFUSIONE: \\n {confusion_matrix(y_true, y_pred)}\")"
      ],
      "metadata": {
        "id": "kzwcLK4CrB75"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(Y_test, clf.predict(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b5SZGCEsZBf",
        "outputId": "83d0478f-e6b9-4211-ef69-6b8de3f66a56"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.967\n",
            "Accuracy: 0.986\n",
            "Precision: 0.983\n",
            "f1 score: 0.975\n",
            "MATRICE DI CONFUSIONE: \n",
            " [[725   5]\n",
            " [ 10 295]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vediamo se il modello presenta overfitting tramite la cross-validation."
      ],
      "metadata": {
        "id": "vojGpSQVGmAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle =True)"
      ],
      "metadata": {
        "id": "8PFNEmn6suly"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creo di nuovo il modello clf per omettere le stampe sulle varie epoche"
      ],
      "metadata": {
        "id": "YGKRS80XOnjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = MLPClassifier(activation='logistic',\n",
        "                    hidden_layer_sizes=(100,),\n",
        "                    max_iter=100,\n",
        "                    solver='adam',\n",
        "                    tol=0.005,\n",
        "                    verbose=False)"
      ],
      "metadata": {
        "id": "-8Hv6kq6Om4Y"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = df['label_num'].values\n",
        "X = np.array(text_cleaned)\n",
        "\n",
        "i=1\n",
        "for train_index , test_index in kf.split(X):\n",
        "\n",
        "    X_train , X_test = X[train_index] , X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "\n",
        "    X_train, tfidf_fitted = bow_tfidf(X_train, None)\n",
        "    X_test, _ = bow_tfidf(X_test, tfidf_fitted)\n",
        "\n",
        "    clf.fit(X_train, Y_train)\n",
        "\n",
        "    print(f\"Metriche dopo {i} split di cross-validation: \\n\")\n",
        "\n",
        "    print(f\"Metriche sul train:\")\n",
        "    evaluate_model(Y_train, clf.predict(X_train))\n",
        "\n",
        "\n",
        "    print(f\"Metriche sul test:\")\n",
        "    evaluate_model(Y_test, clf.predict(X_test))\n",
        "\n",
        "    i+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPWVr3lpGxRj",
        "outputId": "e5ce620e-ce9d-4a6a-ee93-0e500ada2a05"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metriche dopo 1 split di cross-validation: \n",
            "\n",
            "Metriche sul train:\n",
            "Recall: 0.999\n",
            "Accuracy: 1.000\n",
            "Precision: 0.999\n",
            "f1 score: 0.999\n",
            "MATRICE DI CONFUSIONE: \n",
            " [[2944    1]\n",
            " [   1 1190]]\n",
            "Metriche sul test:\n",
            "Recall: 0.990\n",
            "Accuracy: 0.991\n",
            "Precision: 0.981\n",
            "f1 score: 0.985\n",
            "MATRICE DI CONFUSIONE: \n",
            " [[721   6]\n",
            " [  3 305]]\n",
            "Metriche dopo 2 split di cross-validation: \n",
            "\n",
            "Metriche sul train:\n",
            "Recall: 1.000\n",
            "Accuracy: 1.000\n",
            "Precision: 1.000\n",
            "f1 score: 1.000\n",
            "MATRICE DI CONFUSIONE: \n",
            " [[2932    0]\n",
            " [   0 1205]]\n",
            "Metriche sul test:\n",
            "Recall: 0.980\n",
            "Accuracy: 0.985\n",
            "Precision: 0.970\n",
            "f1 score: 0.975\n",
            "MATRICE DI CONFUSIONE: \n",
            " [[731   9]\n",
            " [  6 288]]\n",
            "Metriche dopo 3 split di cross-validation: \n",
            "\n",
            "Metriche sul train:\n",
            "Recall: 0.999\n",
            "Accuracy: 1.000\n",
            "Precision: 0.999\n",
            "f1 score: 0.999\n",
            "MATRICE DI CONFUSIONE: \n",
            " [[2945    1]\n",
            " [   1 1190]]\n",
            "Metriche sul test:\n",
            "Recall: 0.971\n",
            "Accuracy: 0.988\n",
            "Precision: 0.990\n",
            "f1 score: 0.980\n",
            "MATRICE DI CONFUSIONE: \n",
            " [[723   3]\n",
            " [  9 299]]\n",
            "Metriche dopo 4 split di cross-validation: \n",
            "\n",
            "Metriche sul train:\n",
            "Recall: 0.999\n",
            "Accuracy: 1.000\n",
            "Precision: 0.999\n",
            "f1 score: 0.999\n",
            "MATRICE DI CONFUSIONE: \n",
            " [[2923    1]\n",
            " [   1 1212]]\n",
            "Metriche sul test:\n",
            "Recall: 0.976\n",
            "Accuracy: 0.987\n",
            "Precision: 0.979\n",
            "f1 score: 0.977\n",
            "MATRICE DI CONFUSIONE: \n",
            " [[742   6]\n",
            " [  7 279]]\n",
            "Metriche dopo 5 split di cross-validation: \n",
            "\n",
            "Metriche sul train:\n",
            "Recall: 0.999\n",
            "Accuracy: 1.000\n",
            "Precision: 0.999\n",
            "f1 score: 0.999\n",
            "MATRICE DI CONFUSIONE: \n",
            " [[2940    1]\n",
            " [   1 1195]]\n",
            "Metriche sul test:\n",
            "Recall: 0.967\n",
            "Accuracy: 0.986\n",
            "Precision: 0.987\n",
            "f1 score: 0.977\n",
            "MATRICE DI CONFUSIONE: \n",
            " [[727   4]\n",
            " [ 10 293]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Come si può vedere dall'output della cella precedente, non c'è overfitting e il modello ha ottime metriche di accuracy e recall."
      ],
      "metadata": {
        "id": "JRKhcdMS5b2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punto 2**"
      ],
      "metadata": {
        "id": "SGWwPav_QKdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from pprint import pprint\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim import similarities"
      ],
      "metadata": {
        "id": "DxKD-mNQLpAq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selezioniamo le mail di spam:"
      ],
      "metadata": {
        "id": "uoc9uKGZ4pjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_spam = df[df['label_num'] == 1]"
      ],
      "metadata": {
        "id": "q1Ghl_J1RzuS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spam.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyeGm5ADSNq1",
        "outputId": "1ac621df-71b7-46ca-f438-025e2ef87d3a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1499, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggiungiamo un passaggio in più al preprocessing: si può provare ad individuare le parole più frequenti in questo contesto che non sono informative. \\\n",
        "Per individuare le parole poco informative si può scegliere questa strada (simile all'idea che c'è dietro alla metrica tf-idf): attraverso il metodo \"CountVectorizer\" si ottiene una matrice in cui sulle colonne c'è il vocabolario e sulle righe le frasi del dataset. Dunque al posto (i,j) di questa matrice ci sarà il numero di volte che la parola j compare nel documento i. Si riportano tutte le entrate maggiori strettamente di 1 ad 1 (questo perchè non ci interessa quanto frequentemente compare quella parola all'interno del documento) e poi si fa la somma sulle colonne. Con questo approccio si riesce a vedere quali sono le parole che si ripetono più spesso nei vari documenti, che potrebbero essere poco significative."
      ],
      "metadata": {
        "id": "1oKQj91mT9WA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spam_cleaned = data_cleaner(df_spam['text'])\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "counts = count_vectorizer.fit_transform(spam_cleaned)\n",
        "counts[counts > 1] = 1\n",
        "\n",
        "word_counts = np.sum(counts, axis=0)\n",
        "word_counts = np.asarray(word_counts).reshape(-1)\n",
        "\n",
        "vocabulary = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "word_count_dict = {}\n",
        "word_count_dict = dict(zip(vocabulary, word_counts))\n",
        "\n",
        "sorted_word_count = {}\n",
        "sorted_word_count = {k: v for k, v in sorted(word_count_dict.items(), key=lambda item: item[1], reverse=True)}"
      ],
      "metadata": {
        "id": "uEUXS0-YqpDP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A questo punto andiamo a vedere le prime entrate del dizionario \"sorted_word_count\" creato nella cella precedente per eliminare delle parole che non sono significative. Scelgo di verificare manualmente (cioè con un approccio qualitativo) quali sono le parole da eliminare perchè se dovessi ad esempio fissare una soglia e tagliare le parole la cui frequenza è al di sopra di quella soglia rischierei di tagliare delle parole utili."
      ],
      "metadata": {
        "id": "OupNDORo4Ak5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_word_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhxzwgkPAfsX",
        "outputId": "6104af60-1e51-4197-e570-b17800d17e78"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'subject': 1499,\n",
              " 'get': 476,\n",
              " 'http': 475,\n",
              " 'com': 444,\n",
              " 'good': 316,\n",
              " 'please': 314,\n",
              " 'email': 296,\n",
              " 'time': 296,\n",
              " 'price': 295,\n",
              " 'one': 280,\n",
              " 'www': 263,\n",
              " 'new': 253,\n",
              " 'offer': 252,\n",
              " 'need': 250,\n",
              " 'use': 234,\n",
              " 'go': 231,\n",
              " 'want': 225,\n",
              " 'take': 221,\n",
              " 'information': 220,\n",
              " 'click': 216,\n",
              " 'free': 212,\n",
              " 'like': 208,\n",
              " 'product': 208,\n",
              " 'make': 203,\n",
              " 'send': 203,\n",
              " 'look': 190,\n",
              " 'work': 190,\n",
              " 'money': 187,\n",
              " 'online': 185,\n",
              " 'message': 180,\n",
              " 'see': 180,\n",
              " 'know': 178,\n",
              " 'today': 178,\n",
              " 'may': 177,\n",
              " 'order': 177,\n",
              " 'also': 170,\n",
              " 'would': 170,\n",
              " 'thank': 167,\n",
              " 'net': 163,\n",
              " 'receive': 160,\n",
              " 'day': 158,\n",
              " 'include': 157,\n",
              " 'available': 154,\n",
              " 'stop': 154,\n",
              " 'well': 154,\n",
              " 'long': 152,\n",
              " 'mail': 152,\n",
              " 'remove': 152,\n",
              " 'special': 151,\n",
              " 'link': 150,\n",
              " 'low': 150,\n",
              " 'high': 148,\n",
              " 'company': 146,\n",
              " 'give': 146,\n",
              " 'many': 143,\n",
              " 'visit': 141,\n",
              " 'save': 140,\n",
              " 'find': 139,\n",
              " 'service': 134,\n",
              " 'without': 134,\n",
              " 'year': 134,\n",
              " 'list': 132,\n",
              " 'site': 132,\n",
              " 'pay': 129,\n",
              " 'provide': 129,\n",
              " 'result': 129,\n",
              " 'dollar': 128,\n",
              " 'back': 127,\n",
              " 'regard': 127,\n",
              " 'world': 127,\n",
              " 'last': 126,\n",
              " 'business': 125,\n",
              " 'number': 125,\n",
              " 'security': 124,\n",
              " 'come': 121,\n",
              " 'th': 121,\n",
              " 'month': 120,\n",
              " 'show': 120,\n",
              " 'office': 119,\n",
              " 'system': 119,\n",
              " 'act': 118,\n",
              " 'address': 118,\n",
              " 'future': 116,\n",
              " 'great': 116,\n",
              " 'reply': 116,\n",
              " 'buy': 114,\n",
              " 'say': 114,\n",
              " 'could': 113,\n",
              " 'info': 112,\n",
              " 'prescription': 112,\n",
              " 'contact': 111,\n",
              " 'million': 111,\n",
              " 'sale': 110,\n",
              " 'detail': 109,\n",
              " 'software': 109,\n",
              " 'home': 108,\n",
              " 'even': 107,\n",
              " 'man': 105,\n",
              " 'name': 105,\n",
              " 'change': 104,\n",
              " 'increase': 104,\n",
              " 'line': 104,\n",
              " 'within': 104,\n",
              " 'believe': 103,\n",
              " 'copy': 103,\n",
              " 'follow': 103,\n",
              " 'require': 103,\n",
              " 'hour': 102,\n",
              " 'quality': 102,\n",
              " 'right': 102,\n",
              " 'cost': 101,\n",
              " 'help': 100,\n",
              " 'hi': 99,\n",
              " 'internet': 99,\n",
              " 'stock': 99,\n",
              " 'week': 99,\n",
              " 'drug': 98,\n",
              " 'microsoft': 98,\n",
              " 'report': 98,\n",
              " 'first': 97,\n",
              " 'market': 97,\n",
              " 'med': 97,\n",
              " 'phone': 97,\n",
              " 'investment': 96,\n",
              " 'news': 96,\n",
              " 'place': 96,\n",
              " 'less': 95,\n",
              " 'notice': 95,\n",
              " 'professional': 95,\n",
              " 'start': 95,\n",
              " 'try': 95,\n",
              " 'way': 95,\n",
              " 'base': 94,\n",
              " 'customer': 94,\n",
              " 'thousand': 94,\n",
              " 'viagra': 94,\n",
              " 'cheap': 93,\n",
              " 'us': 93,\n",
              " 'check': 92,\n",
              " 'easy': 92,\n",
              " 'every': 92,\n",
              " 'guarantee': 92,\n",
              " 'interest': 92,\n",
              " 'people': 92,\n",
              " 'two': 92,\n",
              " 'via': 92,\n",
              " 'account': 91,\n",
              " 'late': 91,\n",
              " 'much': 91,\n",
              " 'deal': 90,\n",
              " 'feature': 90,\n",
              " 'large': 89,\n",
              " 'title': 88,\n",
              " 'wish': 88,\n",
              " 'call': 87,\n",
              " 'section': 87,\n",
              " 'watch': 87,\n",
              " 'technology': 86,\n",
              " 'html': 85,\n",
              " 'life': 85,\n",
              " 'must': 84,\n",
              " 'paliourg': 84,\n",
              " 'pill': 84,\n",
              " 'real': 84,\n",
              " 'ship': 84,\n",
              " 'doctor': 83,\n",
              " 'php': 83,\n",
              " 'size': 83,\n",
              " 'inc': 82,\n",
              " 'keep': 82,\n",
              " 'sell': 82,\n",
              " 'consider': 81,\n",
              " 'read': 81,\n",
              " 'canada': 79,\n",
              " 'lose': 79,\n",
              " 'pain': 79,\n",
              " 'always': 78,\n",
              " 'feel': 78,\n",
              " 'per': 78,\n",
              " 'private': 78,\n",
              " 'action': 77,\n",
              " 'brand': 77,\n",
              " 'contain': 77,\n",
              " 'forward': 77,\n",
              " 'international': 77,\n",
              " 'rate': 77,\n",
              " 'think': 77,\n",
              " 'full': 76,\n",
              " 'never': 76,\n",
              " 'next': 76,\n",
              " 'project': 76,\n",
              " 'request': 76,\n",
              " 'share': 76,\n",
              " 'solution': 76,\n",
              " 'due': 75,\n",
              " 'note': 75,\n",
              " 'text': 75,\n",
              " 'browser': 74,\n",
              " 'compliance': 74,\n",
              " 'date': 74,\n",
              " 'fax': 74,\n",
              " 'reason': 74,\n",
              " 'risk': 74,\n",
              " 'expect': 73,\n",
              " 'fact': 73,\n",
              " 'intend': 73,\n",
              " 'part': 73,\n",
              " 'party': 73,\n",
              " 'cause': 72,\n",
              " 'cialis': 72,\n",
              " 'complete': 72,\n",
              " 'ever': 72,\n",
              " 'love': 72,\n",
              " 'paste': 72,\n",
              " 'website': 72,\n",
              " 'big': 71,\n",
              " 'fast': 71,\n",
              " 'form': 71,\n",
              " 'source': 71,\n",
              " 'thing': 71,\n",
              " 'let': 70,\n",
              " 'lot': 70,\n",
              " 'performance': 70,\n",
              " 'second': 70,\n",
              " 'term': 70,\n",
              " 'window': 70,\n",
              " 'mobile': 69,\n",
              " 'move': 69,\n",
              " 'release': 69,\n",
              " 'state': 69,\n",
              " 'third': 69,\n",
              " 'biz': 68,\n",
              " 'current': 68,\n",
              " 'medication': 68,\n",
              " 'prior': 68,\n",
              " 'process': 68,\n",
              " 'property': 68,\n",
              " 'top': 68,\n",
              " 'unsubscribe': 68,\n",
              " 'add': 67,\n",
              " 'industry': 67,\n",
              " 'quick': 67,\n",
              " 'secure': 67,\n",
              " 'write': 67,\n",
              " 'invest': 66,\n",
              " 'pass': 66,\n",
              " 'spam': 66,\n",
              " 'woman': 66,\n",
              " 'body': 65,\n",
              " 'content': 65,\n",
              " 'country': 65,\n",
              " 'federal': 65,\n",
              " 'kind': 65,\n",
              " 'little': 65,\n",
              " 'st': 65,\n",
              " 'view': 65,\n",
              " 'area': 64,\n",
              " 'become': 64,\n",
              " 'credit': 64,\n",
              " 'hard': 64,\n",
              " 'minute': 64,\n",
              " 'since': 64,\n",
              " 'hello': 63,\n",
              " 'newsletter': 63,\n",
              " 'position': 63,\n",
              " 'seek': 63,\n",
              " 'sex': 63,\n",
              " 'shipping': 63,\n",
              " 'soft': 63,\n",
              " 'word': 63,\n",
              " 'xp': 63,\n",
              " 'department': 62,\n",
              " 'health': 62,\n",
              " 'statement': 62,\n",
              " 'total': 62,\n",
              " 'trade': 62,\n",
              " 'usa': 62,\n",
              " 'advice': 61,\n",
              " 'cash': 61,\n",
              " 'dealer': 61,\n",
              " 'exchange': 61,\n",
              " 'hot': 61,\n",
              " 'limited': 61,\n",
              " 'past': 61,\n",
              " 'program': 61,\n",
              " 'register': 61,\n",
              " 'set': 61,\n",
              " 'problem': 60,\n",
              " 'approve': 59,\n",
              " 'city': 59,\n",
              " 'commercial': 59,\n",
              " 'hold': 59,\n",
              " 'involve': 59,\n",
              " 'loss': 59,\n",
              " 'super': 59,\n",
              " 'wait': 59,\n",
              " 'advise': 58,\n",
              " 'continue': 58,\n",
              " 'express': 58,\n",
              " 'might': 58,\n",
              " 'press': 58,\n",
              " 'wide': 58,\n",
              " 'discount': 57,\n",
              " 'error': 57,\n",
              " 'event': 57,\n",
              " 'investor': 57,\n",
              " 'meet': 57,\n",
              " 'plan': 57,\n",
              " 'select': 57,\n",
              " 'tel': 57,\n",
              " 'begin': 56,\n",
              " 'bill': 56,\n",
              " 'computer': 56,\n",
              " 'exactly': 56,\n",
              " 'experience': 56,\n",
              " 'format': 56,\n",
              " 'group': 56,\n",
              " 'indicate': 56,\n",
              " 'power': 56,\n",
              " 'purchase': 56,\n",
              " 'still': 56,\n",
              " 'target': 56,\n",
              " 'win': 56,\n",
              " 'worldwide': 56,\n",
              " 'certain': 55,\n",
              " 'control': 55,\n",
              " 'drive': 55,\n",
              " 'effective': 55,\n",
              " 'enter': 55,\n",
              " 'friend': 55,\n",
              " 'identify': 55,\n",
              " 'perfect': 55,\n",
              " 'pharmacy': 55,\n",
              " 'three': 55,\n",
              " 'united': 55,\n",
              " 'zone': 55,\n",
              " 'affiliate': 54,\n",
              " 'application': 54,\n",
              " 'differ': 54,\n",
              " 'end': 54,\n",
              " 'energy': 54,\n",
              " 'growth': 54,\n",
              " 'lead': 54,\n",
              " 'partner': 54,\n",
              " 'plain': 54,\n",
              " 'suite': 54,\n",
              " 'weight': 54,\n",
              " 'dear': 53,\n",
              " 'decide': 53,\n",
              " 'ex': 53,\n",
              " 'fill': 53,\n",
              " 'important': 53,\n",
              " 'instruction': 53,\n",
              " 'nothing': 53,\n",
              " 'video': 53,\n",
              " 'charge': 52,\n",
              " 'creative': 52,\n",
              " 'deliver': 52,\n",
              " 'estimate': 52,\n",
              " 'happy': 52,\n",
              " 'mailing': 52,\n",
              " 'opportunity': 52,\n",
              " 'reader': 52,\n",
              " 'run': 52,\n",
              " 'small': 52,\n",
              " 'turn': 52,\n",
              " 'web': 52,\n",
              " 'xanax': 52,\n",
              " 'additional': 51,\n",
              " 'announce': 51,\n",
              " 'ask': 51,\n",
              " 'design': 51,\n",
              " 'generic': 51,\n",
              " 'image': 51,\n",
              " 'logo': 51,\n",
              " 'mean': 51,\n",
              " 'open': 51,\n",
              " 'profile': 51,\n",
              " 'public': 51,\n",
              " 'remember': 51,\n",
              " 'safe': 51,\n",
              " 'strong': 51,\n",
              " 'transfer': 51,\n",
              " 'type': 51,\n",
              " 'actual': 50,\n",
              " 'ali': 50,\n",
              " 'anticipate': 50,\n",
              " 'girl': 50,\n",
              " 'major': 50,\n",
              " 'management': 50,\n",
              " 'material': 50,\n",
              " 'owner': 50,\n",
              " 'subscriber': 50,\n",
              " 'support': 50,\n",
              " 'tell': 50,\n",
              " 'adobe': 49,\n",
              " 'al': 49,\n",
              " 'canon': 49,\n",
              " 'choose': 49,\n",
              " 'delivery': 49,\n",
              " 'download': 49,\n",
              " 'effort': 49,\n",
              " 'plus': 49,\n",
              " 'understand': 49,\n",
              " 'age': 48,\n",
              " 'box': 48,\n",
              " 'draw': 48,\n",
              " 'gain': 48,\n",
              " 'hear': 48,\n",
              " 'huge': 48,\n",
              " 'ibm': 48,\n",
              " 'medical': 48,\n",
              " 'photoshop': 48,\n",
              " 'possible': 48,\n",
              " 'prove': 48,\n",
              " 'respect': 48,\n",
              " 'review': 48,\n",
              " 'uncertainty': 48,\n",
              " 'value': 48,\n",
              " 'version': 48,\n",
              " 'america': 47,\n",
              " 'availability': 47,\n",
              " 'financial': 47,\n",
              " 'hundred': 47,\n",
              " 'iii': 47,\n",
              " 'limit': 47,\n",
              " 'reach': 47,\n",
              " 'trademark': 47,\n",
              " 'update': 47,\n",
              " 'valium': 47,\n",
              " 'allow': 46,\n",
              " 'bank': 46,\n",
              " 'door': 46,\n",
              " 'duty': 46,\n",
              " 'government': 46,\n",
              " 'half': 46,\n",
              " 'interested': 46,\n",
              " 'learn': 46,\n",
              " 'payment': 46,\n",
              " 'represent': 46,\n",
              " 'search': 46,\n",
              " 'user': 46,\n",
              " 'yahoo': 46,\n",
              " 'able': 45,\n",
              " 'currently': 45,\n",
              " 'dell': 45,\n",
              " 'erection': 45,\n",
              " 'fix': 45,\n",
              " 'gr': 45,\n",
              " 'meaning': 45,\n",
              " 'necessary': 45,\n",
              " 'none': 45,\n",
              " 'north': 45,\n",
              " 'publication': 45,\n",
              " 'cd': 44,\n",
              " 'clearance': 44,\n",
              " 'director': 44,\n",
              " 'everything': 44,\n",
              " 'export': 44,\n",
              " 'fund': 44,\n",
              " 'original': 44,\n",
              " 'popular': 44,\n",
              " 'pro': 44,\n",
              " 'respective': 44,\n",
              " 'somehow': 44,\n",
              " 'sony': 44,\n",
              " 'states': 44,\n",
              " 'access': 43,\n",
              " 'aware': 43,\n",
              " 'build': 43,\n",
              " 'confirm': 43,\n",
              " 'congress': 43,\n",
              " 'epson': 43,\n",
              " 'leave': 43,\n",
              " 'live': 43,\n",
              " 'natural': 43,\n",
              " 'operation': 43,\n",
              " 'paragraph': 43,\n",
              " 'really': 43,\n",
              " 'reseller': 43,\n",
              " 'solicitation': 43,\n",
              " 'store': 43,\n",
              " 'already': 42,\n",
              " 'complaint': 42,\n",
              " 'create': 42,\n",
              " 'enjoy': 42,\n",
              " 'grow': 42,\n",
              " 'immediately': 42,\n",
              " 'member': 42,\n",
              " 'network': 42,\n",
              " 'oo': 42,\n",
              " 'pertain': 42,\n",
              " 'rx': 42,\n",
              " 'server': 42,\n",
              " 'side': 42,\n",
              " 'soon': 42,\n",
              " 'supply': 42,\n",
              " 'tab': 42,\n",
              " 'accept': 41,\n",
              " 'advisor': 41,\n",
              " 'around': 41,\n",
              " 'bad': 41,\n",
              " 'carry': 41,\n",
              " 'compare': 41,\n",
              " 'computron': 41,\n",
              " 'enquiry': 41,\n",
              " 'head': 41,\n",
              " 'hewlett': 41,\n",
              " 'history': 41,\n",
              " 'intel': 41,\n",
              " 'jebel': 41,\n",
              " 'legislation': 41,\n",
              " 'license': 41,\n",
              " 'near': 41,\n",
              " 'objective': 41,\n",
              " 'representative': 41,\n",
              " 'retail': 41,\n",
              " 'robotic': 41,\n",
              " 'shall': 41,\n",
              " 'sign': 41,\n",
              " 'simply': 41,\n",
              " 'suggestion': 41,\n",
              " 'upon': 41,\n",
              " 'aopen': 40,\n",
              " 'apc': 40,\n",
              " 'assumption': 40,\n",
              " 'card': 40,\n",
              " 'cisco': 40,\n",
              " 'compaq': 40,\n",
              " 'customerservice': 40,\n",
              " 'develop': 40,\n",
              " 'expectation': 40,\n",
              " 'four': 40,\n",
              " 'goal': 40,\n",
              " 'herein': 40,\n",
              " 'inherent': 40,\n",
              " 'intellinet': 40,\n",
              " 'iomega': 40,\n",
              " 'occur': 40,\n",
              " 'opinion': 40,\n",
              " 'packard': 40,\n",
              " 'recent': 40,\n",
              " 'sexual': 40,\n",
              " 'talk': 40,\n",
              " 'targus': 40,\n",
              " 'toshiba': 40,\n",
              " 'unique': 40,\n",
              " 'viewsonic': 40,\n",
              " 'young': 40,\n",
              " 'break': 39,\n",
              " 'communication': 39,\n",
              " 'conflict': 39,\n",
              " 'construe': 39,\n",
              " 'demokrito': 39,\n",
              " 'discussion': 39,\n",
              " 'effect': 39,\n",
              " 'emirate': 39,\n",
              " 'exclusive': 39,\n",
              " 'gra': 39,\n",
              " 'happen': 39,\n",
              " 'iit': 39,\n",
              " 'letter': 39,\n",
              " 'level': 39,\n",
              " 'old': 39,\n",
              " 'play': 39,\n",
              " 'presently': 39,\n",
              " 'purpose': 39,\n",
              " 'relief': 39,\n",
              " 'wife': 39,\n",
              " 'bring': 38,\n",
              " 'case': 38,\n",
              " 'condition': 38,\n",
              " 'constitute': 38,\n",
              " 'cs': 38,\n",
              " 'direct': 38,\n",
              " 'potential': 38,\n",
              " 'put': 38,\n",
              " 'question': 38,\n",
              " 'record': 38,\n",
              " 'sure': 38,\n",
              " 'uae': 38,\n",
              " 'aeor': 37,\n",
              " 'care': 37,\n",
              " 'co': 37,\n",
              " 'concern': 37,\n",
              " 'corel': 37,\n",
              " 'development': 37,\n",
              " 'everyone': 37,\n",
              " 'explode': 37,\n",
              " 'face': 37,\n",
              " 'graphic': 37,\n",
              " 'href': 37,\n",
              " 'improve': 37,\n",
              " 'inch': 37,\n",
              " 'matter': 37,\n",
              " 'mind': 37,\n",
              " 'officer': 37,\n",
              " 'projection': 37,\n",
              " 'shop': 37,\n",
              " 'sleep': 37,\n",
              " 'zonedubai': 37,\n",
              " 'away': 36,\n",
              " 'hope': 36,\n",
              " 'manager': 36,\n",
              " 'produce': 36,\n",
              " 'revenue': 36,\n",
              " 'short': 36,\n",
              " 'sincerely': 36,\n",
              " 'ability': 35,\n",
              " 'accuracy': 35,\n",
              " 'alert': 35,\n",
              " 'another': 35,\n",
              " 'belief': 35,\n",
              " 'center': 35,\n",
              " 'datum': 35,\n",
              " 'file': 35,\n",
              " 'hey': 35,\n",
              " 'key': 35,\n",
              " 'medium': 35,\n",
              " 'option': 35,\n",
              " 'personal': 35,\n",
              " 'photo': 35,\n",
              " 'prediction': 35,\n",
              " 'profit': 35,\n",
              " 'ready': 35,\n",
              " 'relate': 35,\n",
              " 'rm': 35,\n",
              " 'symbol': 35,\n",
              " 'weekend': 35,\n",
              " 'ad': 34,\n",
              " 'attention': 34,\n",
              " 'earn': 34,\n",
              " 'enhance': 34,\n",
              " 'enough': 34,\n",
              " 'expense': 34,\n",
              " 'foresee': 34,\n",
              " 'general': 34,\n",
              " 'immediate': 34,\n",
              " 'index': 34,\n",
              " 'january': 34,\n",
              " 'materially': 34,\n",
              " 'mr': 34,\n",
              " 'page': 34,\n",
              " 'pc': 34,\n",
              " 'quickly': 34,\n",
              " 'return': 34,\n",
              " 'sit': 34,\n",
              " 'soma': 34,\n",
              " 'strategy': 34,\n",
              " 'worth': 34,\n",
              " 'acquire': 33,\n",
              " 'affordable': 33,\n",
              " 'ago': 33,\n",
              " 'agreement': 33,\n",
              " 'anti': 33,\n",
              " 'benefit': 33,\n",
              " 'book': 33,\n",
              " 'claim': 33,\n",
              " 'corporation': 33,\n",
              " 'factor': 33,\n",
              " 'family': 33,\n",
              " 'far': 33,\n",
              " 'issue': 33,\n",
              " 'load': 33,\n",
              " 'min': 33,\n",
              " 'overnight': 33,\n",
              " 'point': 33,\n",
              " 'publisher': 33,\n",
              " 'reduce': 33,\n",
              " 'rock': 33,\n",
              " 'stand': 33,\n",
              " 'treatment': 33,\n",
              " 'advantage': 32,\n",
              " 'charset': 32,\n",
              " 'child': 32,\n",
              " 'close': 32,\n",
              " 'eye': 32,\n",
              " 'five': 32,\n",
              " 'highly': 32,\n",
              " 'join': 32,\n",
              " 'movie': 32,\n",
              " 'normal': 32,\n",
              " 'oil': 32,\n",
              " 'penis': 32,\n",
              " 'simple': 32,\n",
              " 'south': 32,\n",
              " 'test': 32,\n",
              " 'transaction': 32,\n",
              " 'broker': 31,\n",
              " 'color': 31,\n",
              " 'consumer': 31,\n",
              " 'contract': 31,\n",
              " 'deposit': 31,\n",
              " 'example': 31,\n",
              " 'flash': 31,\n",
              " 'font': 31,\n",
              " 'generate': 31,\n",
              " 'house': 31,\n",
              " 'muscle': 31,\n",
              " 'night': 31,\n",
              " 'operate': 31,\n",
              " 'picture': 31,\n",
              " 'quarter': 31,\n",
              " 'reference': 31,\n",
              " 'reliable': 31,\n",
              " 'shareholder': 31,\n",
              " 'speculative': 31,\n",
              " 'trading': 31,\n",
              " 'various': 31,\n",
              " 'atleast': 30,\n",
              " 'bit': 30,\n",
              " 'blank': 30,\n",
              " 'br': 30,\n",
              " 'cover': 30,\n",
              " 'digital': 30,\n",
              " 'enterprise': 30,\n",
              " 'fun': 30,\n",
              " 'leader': 30,\n",
              " 'local': 30,\n",
              " 'marketing': 30,\n",
              " 'someone': 30,\n",
              " 'success': 30,\n",
              " 'sun': 30,\n",
              " 'assist': 29,\n",
              " 'cap': 29,\n",
              " 'car': 29,\n",
              " 'ce': 29,\n",
              " 'corporate': 29,\n",
              " 'database': 29,\n",
              " 'die': 29,\n",
              " 'diligence': 29,\n",
              " 'discover': 29,\n",
              " 'drink': 29,\n",
              " 'ed': 29,\n",
              " 'either': 29,\n",
              " 'law': 29,\n",
              " 'lie': 29,\n",
              " 'mix': 29,\n",
              " 'national': 29,\n",
              " 'positive': 29,\n",
              " 'premium': 29,\n",
              " 'privacy': 29,\n",
              " 'quote': 29,\n",
              " 'remain': 29,\n",
              " 'sum': 29,\n",
              " 'td': 29,\n",
              " 'treat': 29,\n",
              " 'vicodin': 29,\n",
              " 'windows': 29,\n",
              " 'acceptance': 28,\n",
              " 'advanced': 28,\n",
              " 'advertisement': 28,\n",
              " 'agree': 28,\n",
              " 'channel': 28,\n",
              " 'code': 28,\n",
              " 'compensation': 28,\n",
              " 'consultation': 28,\n",
              " 'corp': 28,\n",
              " 'de': 28,\n",
              " 'disclose': 28,\n",
              " 'exist': 28,\n",
              " 'expert': 28,\n",
              " 'gather': 28,\n",
              " 'legal': 28,\n",
              " 'official': 28,\n",
              " 'outstanding': 28,\n",
              " 'person': 28,\n",
              " 'powerful': 28,\n",
              " 'production': 28,\n",
              " 'range': 28,\n",
              " 'response': 28,\n",
              " 'rolex': 28,\n",
              " 'saving': 28,\n",
              " 'similar': 28,\n",
              " 'something': 28,\n",
              " 'tax': 28,\n",
              " 'voice': 28,\n",
              " 'acquisition': 27,\n",
              " 'demand': 27,\n",
              " 'discreet': 27,\n",
              " 'document': 27,\n",
              " 'etc': 27,\n",
              " 'exercise': 27,\n",
              " 'fat': 27,\n",
              " 'fee': 27,\n",
              " 'foreign': 27,\n",
              " 'kill': 27,\n",
              " 'obtain': 27,\n",
              " 'penny': 27,\n",
              " 'pick': 27,\n",
              " 'prozac': 27,\n",
              " 'red': 27,\n",
              " 'sec': 27,\n",
              " 'tongue': 27,\n",
              " 'vi': 27,\n",
              " 'wi': 27,\n",
              " 'almost': 26,\n",
              " 'answer': 26,\n",
              " 'approval': 26,\n",
              " 'capital': 26,\n",
              " 'category': 26,\n",
              " 'chance': 26,\n",
              " 'confidential': 26,\n",
              " 'course': 26,\n",
              " 'distributorjebel': 26,\n",
              " 'edition': 26,\n",
              " 'emerge': 26,\n",
              " 'expand': 26,\n",
              " 'extra': 26,\n",
              " 'finally': 26,\n",
              " 'friday': 26,\n",
              " 'front': 26,\n",
              " 'historical': 26,\n",
              " 'htm': 26,\n",
              " 'individual': 26,\n",
              " 'inside': 26,\n",
              " 'jan': 26,\n",
              " 'light': 26,\n",
              " 'manage': 26,\n",
              " 'morning': 26,\n",
              " 'pack': 26,\n",
              " 'president': 26,\n",
              " 'protect': 26,\n",
              " 'raise': 26,\n",
              " 'receipt': 26,\n",
              " 'recommend': 26,\n",
              " 'se': 26,\n",
              " 'standard': 26,\n",
              " 'team': 26,\n",
              " 'telephone': 26,\n",
              " 'tr': 26,\n",
              " 'yet': 26,\n",
              " 'active': 25,\n",
              " 'ambien': 25,\n",
              " 'among': 25,\n",
              " 'boost': 25,\n",
              " 'bottom': 25,\n",
              " 'cable': 25,\n",
              " 'compensate': 25,\n",
              " 'different': 25,\n",
              " 'early': 25,\n",
              " 'easily': 25,\n",
              " 'entertainment': 25,\n",
              " 'equity': 25,\n",
              " 'forget': 25,\n",
              " 'fwd': 25,\n",
              " 'green': 25,\n",
              " 'heart': 25,\n",
              " 'location': 25,\n",
              " 'mai': 25,\n",
              " 'massive': 25,\n",
              " 'mention': 25,\n",
              " 'ms': 25,\n",
              " 'multi': 25,\n",
              " 'multiple': 25,\n",
              " 'mx': 25,\n",
              " 'non': 25,\n",
              " 'notification': 25,\n",
              " 'oem': 25,\n",
              " 'operating': 25,\n",
              " 'pre': 25,\n",
              " 'reliance': 25,\n",
              " 'single': 25,\n",
              " 'therefore': 25,\n",
              " 'though': 25,\n",
              " 'tool': 25,\n",
              " 'accounting': 24,\n",
              " 'aggressive': 24,\n",
              " 'alcohol': 24,\n",
              " 'appear': 24,\n",
              " 'apply': 24,\n",
              " 'assistance': 24,\n",
              " 'assure': 24,\n",
              " 'avoid': 24,\n",
              " 'award': 24,\n",
              " 'border': 24,\n",
              " 'campaign': 24,\n",
              " 'clear': 24,\n",
              " 'comfort': 24,\n",
              " 'connection': 24,\n",
              " 'directly': 24,\n",
              " 'distribute': 24,\n",
              " 'dr': 24,\n",
              " 'drop': 24,\n",
              " 'entire': 24,\n",
              " 'fda': 24,\n",
              " 'fully': 24,\n",
              " 'later': 24,\n",
              " 'march': 24,\n",
              " 'norton': 24,\n",
              " 'omit': 24,\n",
              " 'rd': 24,\n",
              " 'requirement': 24,\n",
              " 'self': 24,\n",
              " 'sir': 24,\n",
              " 'speed': 24,\n",
              " 'stay': 24,\n",
              " 'studio': 24,\n",
              " 'white': 24,\n",
              " 'adult': 23,\n",
              " 'alternative': 23,\n",
              " 'anxiety': 23,\n",
              " 'anything': 23,\n",
              " 'asset': 23,\n",
              " 'bankruptcy': 23,\n",
              " 'burn': 23,\n",
              " 'ceo': 23,\n",
              " 'cia': 23,\n",
              " 'dec': 23,\n",
              " 'device': 23,\n",
              " 'eight': 23,\n",
              " 'en': 23,\n",
              " 'european': 23,\n",
              " 'expensive': 23,\n",
              " 'filing': 23,\n",
              " 'financing': 23,\n",
              " 'focus': 23,\n",
              " 'impotence': 23,\n",
              " 'intent': 23,\n",
              " 'las': 23,\n",
              " 'loan': 23,\n",
              " 'locate': 23,\n",
              " 'macromedia': 23,\n",
              " 'membership': 23,\n",
              " 'mg': 23,\n",
              " 'microcap': 23,\n",
              " 'ok': 23,\n",
              " 'pleasure': 23,\n",
              " 'present': 23,\n",
              " 'provider': 23,\n",
              " 'publish': 23,\n",
              " 'ra': 23,\n",
              " 'recently': 23,\n",
              " 'reduction': 23,\n",
              " 'registered': 23,\n",
              " 'research': 23,\n",
              " 'son': 23,\n",
              " 'spend': 23,\n",
              " 'star': 23,\n",
              " 'story': 23,\n",
              " 'therein': 23,\n",
              " 'vlagra': 23,\n",
              " 'africa': 22,\n",
              " 'air': 22,\n",
              " 'american': 22,\n",
              " 'bias': 22,\n",
              " 'competition': 22,\n",
              " 'decision': 22,\n",
              " 'east': 22,\n",
              " 'field': 22,\n",
              " 'foot': 22,\n",
              " 'force': 22,\n",
              " 'forever': 22,\n",
              " 'formula': 22,\n",
              " 'gas': 22,\n",
              " 'hand': 22,\n",
              " 'hassle': 22,\n",
              " 'host': 22,\n",
              " 'idea': 22,\n",
              " 'illustrator': 22,\n",
              " 'inform': 22,\n",
              " 'ingredient': 22,\n",
              " 'introduce': 22,\n",
              " 'mo': 22,\n",
              " 'monday': 22,\n",
              " 'music': 22,\n",
              " 'nd': 22,\n",
              " 'ng': 22,\n",
              " 'portfolio': 22,\n",
              " 'premiere': 22,\n",
              " 'qualify': 22,\n",
              " 'six': 22,\n",
              " 'step': 22,\n",
              " 'table': 22,\n",
              " 'tag': 22,\n",
              " 'trust': 22,\n",
              " 'vegas': 22,\n",
              " 'verge': 22,\n",
              " 'wild': 22,\n",
              " 'achieve': 21,\n",
              " 'amazing': 21,\n",
              " 'anywhere': 21,\n",
              " 'billion': 21,\n",
              " 'black': 21,\n",
              " 'camera': 21,\n",
              " 'choice': 21,\n",
              " 'client': 21,\n",
              " 'completeness': 21,\n",
              " 'confidentiality': 21,\n",
              " 'confirmation': 21,\n",
              " 'daily': 21,\n",
              " 'double': 21,\n",
              " 'forth': 21,\n",
              " 'game': 21,\n",
              " 'indicative': 21,\n",
              " 'iso': 21,\n",
              " 'male': 21,\n",
              " 'miss': 21,\n",
              " 'mortgage': 21,\n",
              " 'opt': 21,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tra le parole che compaiono più spesso possiamo raggruppare in una lista quelle che sembrano meno informative."
      ],
      "metadata": {
        "id": "YpnABAi9v1VW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_informative = ['subject','get','http','com','please','good','email','time', 'one','www','need','new','go','take','want','click','like','make','look','thank',\n",
        "                    'message','may','also','would','see','know','receive']"
      ],
      "metadata": {
        "id": "buvwCaUcxe2s"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A questo punto possiamo usare il preprocessing fornito da gensim e poi allargare la classe delle stopwords con la lista \"non_informative\" e rimuovere stopwords e parole con lunghezza minore o uguale a 2. Quest'ultima cosa è dovuta al fatto che la parola \"com\" compariva molto spesso e quindi intuitivamente potrebbero esserci dei messaggi di spam che rimandano a siti web e quindi possiamo trovare dei \"it\", \"de\", \"en\" e cosi via."
      ],
      "metadata": {
        "id": "PqS4xSiVw3gM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_stopwords.extend(non_informative)\n",
        "\n",
        "def sent_to_words(dataset):\n",
        "    for sentence in dataset:\n",
        "        yield gensim.utils.simple_preprocess(str(sentence), deacc=True)\n",
        "\n",
        "def remove_stopwords(dataset):\n",
        "  for sentence in dataset:\n",
        "    yield [word for word in sentence.split() if word not in english_stopwords and len(word)>2]"
      ],
      "metadata": {
        "id": "KBGk7ZJdSOv-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_words = list(sent_to_words(list(remove_stopwords(spam_cleaned))))"
      ],
      "metadata": {
        "id": "298H2k9t5wXU"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2word = corpora.Dictionary(data_words)\n",
        "\n",
        "corpus = [id2word.doc2bow(text) for text in data_words]"
      ],
      "metadata": {
        "id": "4DVuWmU6TQ5U"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora, il numero di topics è un parametro di cui fare il tuning, così come il parametro \"passes\"; non avendo problemi dal punto di vista computazionale visto che il dataset non è molto grande possiamo impostare passes a 10 e proviamo con 3 topics."
      ],
      "metadata": {
        "id": "YJ5EQ_JtS67n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_topics = 3\n",
        "\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=num_topics,\n",
        "                                       passes = 10,\n",
        "                                       random_state=30 )\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW_N79oxTU5C",
        "outputId": "af208dfb-783e-4b3b-9faa-e7406713db9b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.005*\"price\" + 0.004*\"contact\" + 0.004*\"free\" + 0.004*\"computron\" + '\n",
            "  '0.003*\"send\" + 0.003*\"remove\" + 0.003*\"mail\" + 0.003*\"account\" + '\n",
            "  '0.003*\"money\" + 0.003*\"offer\"'),\n",
            " (1,\n",
            "  '0.007*\"pill\" + 0.004*\"viagra\" + 0.003*\"cialis\" + 0.003*\"prescription\" + '\n",
            "  '0.003*\"online\" + 0.002*\"drug\" + 0.002*\"med\" + 0.002*\"order\" + 0.002*\"soft\" '\n",
            "  '+ 0.002*\"save\"'),\n",
            " (2,\n",
            "  '0.012*\"company\" + 0.007*\"font\" + 0.007*\"statement\" + 0.006*\"stock\" + '\n",
            "  '0.006*\"nbsp\" + 0.005*\"height\" + 0.005*\"information\" + 0.005*\"security\" + '\n",
            "  '0.004*\"price\" + 0.004*\"width\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Già con num_topics=3 si ottiene un buon livello di coerenza."
      ],
      "metadata": {
        "id": "YI5YRKILzwgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per fare tuning del parametro num_topics scegliamo un insieme di candidati e andiamo a valutare per ogni candidato la coherence (che misura appunto la coerenza delle parole all'interno di un topic) del modello LDA ottenuto."
      ],
      "metadata": {
        "id": "VvgY8_7Bz7bL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_best_model(list_num_topics):\n",
        "\n",
        "    text = list(sent_to_words(list(remove_stopwords(data_cleaner(df_spam['text'])))))\n",
        "    coherence = []\n",
        "    models = []\n",
        "\n",
        "    for num_topics in list_num_topics:\n",
        "        lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                              id2word=id2word,\n",
        "                                              num_topics=num_topics,\n",
        "                                              passes = 10,\n",
        "                                              random_state = 50)\n",
        "        models.append(lda_model)\n",
        "\n",
        "        coherence_model_lda = CoherenceModel(model=lda_model, texts=text, dictionary=id2word, coherence='c_v')\n",
        "        coherence.append(coherence_model_lda.get_coherence())\n",
        "\n",
        "    max_index = coherence.index(max(coherence))\n",
        "    selected_model = models[max_index]\n",
        "\n",
        "    pprint(selected_model.print_topics())\n",
        "    print(max(coherence))\n",
        "    return selected_model, coherence"
      ],
      "metadata": {
        "id": "Eesez3wdUUo9"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, coherence = select_best_model([3,4,5,6,7,8,9,10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwXh7g7e0zIV",
        "outputId": "367a6060-01da-43d1-c475-52a0e559ebe8"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.011*\"font\" + 0.009*\"computron\" + 0.007*\"contact\" + 0.007*\"height\" + '\n",
            "  '0.006*\"remove\" + 0.006*\"size\" + 0.005*\"free\" + 0.005*\"price\" + 0.005*\"line\" '\n",
            "  '+ 0.005*\"list\"'),\n",
            " (1,\n",
            "  '0.007*\"account\" + 0.005*\"money\" + 0.004*\"number\" + 0.004*\"international\" + '\n",
            "  '0.004*\"claim\" + 0.003*\"business\" + 0.003*\"bank\" + 0.003*\"security\" + '\n",
            "  '0.003*\"call\" + 0.003*\"mail\"'),\n",
            " (2,\n",
            "  '0.015*\"nbsp\" + 0.008*\"height\" + 0.007*\"width\" + 0.005*\"border\" + '\n",
            "  '0.004*\"soft\" + 0.004*\"back\" + 0.004*\"cialis\" + 0.004*\"well\" + 0.004*\"find\" '\n",
            "  '+ 0.004*\"rate\"'),\n",
            " (3,\n",
            "  '0.017*\"company\" + 0.010*\"statement\" + 0.009*\"stock\" + 0.007*\"information\" + '\n",
            "  '0.006*\"security\" + 0.006*\"report\" + 0.006*\"investment\" + 0.005*\"within\" + '\n",
            "  '0.005*\"price\" + 0.004*\"inc\"'),\n",
            " (4,\n",
            "  '0.012*\"price\" + 0.008*\"window\" + 0.008*\"adobe\" + 0.006*\"software\" + '\n",
            "  '0.006*\"professional\" + 0.005*\"office\" + 0.005*\"save\" + 0.004*\"microsoft\" + '\n",
            "  '0.004*\"retail\" + 0.004*\"photoshop\"'),\n",
            " (5,\n",
            "  '0.002*\"dosage\" + 0.002*\"die\" + 0.001*\"think\" + 0.001*\"der\" + 0.001*\"stock\" '\n",
            "  '+ 0.001*\"without\" + 0.001*\"female\" + 0.001*\"day\" + 0.001*\"buzz\" + '\n",
            "  '0.001*\"bronco\"'),\n",
            " (6,\n",
            "  '0.016*\"pill\" + 0.004*\"viagra\" + 0.004*\"online\" + 0.004*\"prescription\" + '\n",
            "  '0.003*\"valium\" + 0.003*\"xanax\" + 0.003*\"content\" + 0.003*\"med\" + '\n",
            "  '0.003*\"health\" + 0.002*\"html\"'),\n",
            " (7,\n",
            "  '0.011*\"font\" + 0.008*\"color\" + 0.008*\"face\" + 0.006*\"align\" + 0.005*\"size\" '\n",
            "  '+ 0.003*\"strong\" + 0.003*\"div\" + 0.003*\"darial\" + 0.003*\"info\" + '\n",
            "  '0.002*\"fontfont\"')]\n",
            "0.5717848323427552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punto 3**"
      ],
      "metadata": {
        "id": "E1OMXmsIDVD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per calcolare la distanza semantica tra i topics individuati al punto precedente, l'idea è quella di prendere un rappresentante per ogni topic e poi calcolare la cosine similarity tra i rappresentanti di ogni topic. \\\n",
        "Fissato un topic: \\\n",
        "1) scegliamo le parole più rilevanti (più probabili) associate a quel topic; \\\n",
        "2) calcoliamo la rappresentazione Word2Vec delle parole scelte al punto precedente; \\\n",
        "3) calcoliamo un vettore di media (elemento per elemento) dei vettori ottenuti al punto precedente (questo sarà il rappresentante del topic fissato). \\\n",
        "Iteriamo poi i 3 punti per ogni topic."
      ],
      "metadata": {
        "id": "R3pjFYC3aLYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader\n",
        "from scipy import spatial"
      ],
      "metadata": {
        "id": "4IKR6h77GhC7"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86NatR6fGmWn",
        "outputId": "83287926-cbf3-4858-f054-e55f7bf33f45"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_representatives = []\n",
        "for topic_id in range(model.num_topics):\n",
        "    topic_words = [word for word, _ in model.show_topic(topic_id)]\n",
        "    topic_embeddings = [glove_vectors[word] for word in topic_words if word in glove_vectors]\n",
        "    if topic_embeddings:\n",
        "        topic_centroid = sum(topic_embeddings) / len(topic_embeddings)\n",
        "        topic_representatives.append(topic_centroid)\n",
        "    else:\n",
        "        topic_representatives.append(None)"
      ],
      "metadata": {
        "id": "X1iMGch6GnGR"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A questo punto abbiamo i rappresentanti per ogni topic, nella cella seguente andiamo a calcolare le distanze tra i topic."
      ],
      "metadata": {
        "id": "CywsPJHncl4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distances = []\n",
        "for i in range(len(topic_representatives)):\n",
        "   d = []\n",
        "   for j in range(len(topic_representatives)):\n",
        "      if i != j:\n",
        "        d.append(1-spatial.distance.cosine(topic_representatives[i], topic_representatives[j]))\n",
        "   distances.append(d)"
      ],
      "metadata": {
        "id": "khUPPJGNOGwD"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tramite la cella seguente andiamo a visualizzare le distanze semantiche individuate."
      ],
      "metadata": {
        "id": "_rV38IdgcuWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(distances)):\n",
        "  for j in range(len(distances[i])):\n",
        "    if j < i:\n",
        "      print(f'Distanza semantica del topic {i} dal topic {j}: {distances[i][j]}')\n",
        "    else:\n",
        "      print(f'Distanza semantica del topic {i} dal topic {j+1}: {distances[i][j]}')\n",
        "  print(\"------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7adhXtoISJmU",
        "outputId": "641b0f01-ac56-4ada-b814-f4df9016e189"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distanza semantica del topic 0 dal topic 1: 0.570235550403595\n",
            "Distanza semantica del topic 0 dal topic 2: 0.6763640642166138\n",
            "Distanza semantica del topic 0 dal topic 3: 0.5634257197380066\n",
            "Distanza semantica del topic 0 dal topic 4: 0.5360941290855408\n",
            "Distanza semantica del topic 0 dal topic 5: 0.5423681735992432\n",
            "Distanza semantica del topic 0 dal topic 6: 0.3063722550868988\n",
            "Distanza semantica del topic 0 dal topic 7: 0.6138285994529724\n",
            "------------------\n",
            "Distanza semantica del topic 1 dal topic 0: 0.570235550403595\n",
            "Distanza semantica del topic 1 dal topic 2: 0.5443044304847717\n",
            "Distanza semantica del topic 1 dal topic 3: 0.7978641390800476\n",
            "Distanza semantica del topic 1 dal topic 4: 0.5355232954025269\n",
            "Distanza semantica del topic 1 dal topic 5: 0.5377824902534485\n",
            "Distanza semantica del topic 1 dal topic 6: 0.24029801785945892\n",
            "Distanza semantica del topic 1 dal topic 7: 0.3765726387500763\n",
            "------------------\n",
            "Distanza semantica del topic 2 dal topic 0: 0.6763640642166138\n",
            "Distanza semantica del topic 2 dal topic 1: 0.5443044304847717\n",
            "Distanza semantica del topic 2 dal topic 3: 0.4936201870441437\n",
            "Distanza semantica del topic 2 dal topic 4: 0.4036500155925751\n",
            "Distanza semantica del topic 2 dal topic 5: 0.5329374074935913\n",
            "Distanza semantica del topic 2 dal topic 6: 0.2726574242115021\n",
            "Distanza semantica del topic 2 dal topic 7: 0.5073739886283875\n",
            "------------------\n",
            "Distanza semantica del topic 3 dal topic 0: 0.5634257197380066\n",
            "Distanza semantica del topic 3 dal topic 1: 0.7978641390800476\n",
            "Distanza semantica del topic 3 dal topic 2: 0.4936201870441437\n",
            "Distanza semantica del topic 3 dal topic 4: 0.5916016101837158\n",
            "Distanza semantica del topic 3 dal topic 5: 0.5234623551368713\n",
            "Distanza semantica del topic 3 dal topic 6: 0.25904911756515503\n",
            "Distanza semantica del topic 3 dal topic 7: 0.37939587235450745\n",
            "------------------\n",
            "Distanza semantica del topic 4 dal topic 0: 0.5360941290855408\n",
            "Distanza semantica del topic 4 dal topic 1: 0.5355232954025269\n",
            "Distanza semantica del topic 4 dal topic 2: 0.4036500155925751\n",
            "Distanza semantica del topic 4 dal topic 3: 0.5916016101837158\n",
            "Distanza semantica del topic 4 dal topic 5: 0.41317418217658997\n",
            "Distanza semantica del topic 4 dal topic 6: 0.35289356112480164\n",
            "Distanza semantica del topic 4 dal topic 7: 0.4615205228328705\n",
            "------------------\n",
            "Distanza semantica del topic 5 dal topic 0: 0.5423681735992432\n",
            "Distanza semantica del topic 5 dal topic 1: 0.5377824902534485\n",
            "Distanza semantica del topic 5 dal topic 2: 0.5329374074935913\n",
            "Distanza semantica del topic 5 dal topic 3: 0.5234623551368713\n",
            "Distanza semantica del topic 5 dal topic 4: 0.41317418217658997\n",
            "Distanza semantica del topic 5 dal topic 6: 0.34396663308143616\n",
            "Distanza semantica del topic 5 dal topic 7: 0.3542732298374176\n",
            "------------------\n",
            "Distanza semantica del topic 6 dal topic 0: 0.3063722550868988\n",
            "Distanza semantica del topic 6 dal topic 1: 0.24029801785945892\n",
            "Distanza semantica del topic 6 dal topic 2: 0.2726574242115021\n",
            "Distanza semantica del topic 6 dal topic 3: 0.25904911756515503\n",
            "Distanza semantica del topic 6 dal topic 4: 0.35289356112480164\n",
            "Distanza semantica del topic 6 dal topic 5: 0.34396663308143616\n",
            "Distanza semantica del topic 6 dal topic 7: 0.19932463765144348\n",
            "------------------\n",
            "Distanza semantica del topic 7 dal topic 0: 0.6138285994529724\n",
            "Distanza semantica del topic 7 dal topic 1: 0.3765726387500763\n",
            "Distanza semantica del topic 7 dal topic 2: 0.5073739886283875\n",
            "Distanza semantica del topic 7 dal topic 3: 0.37939587235450745\n",
            "Distanza semantica del topic 7 dal topic 4: 0.4615205228328705\n",
            "Distanza semantica del topic 7 dal topic 5: 0.3542732298374176\n",
            "Distanza semantica del topic 7 dal topic 6: 0.19932463765144348\n",
            "------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Come si può vedere dall'output della cella precedente, i topic sono ben distinti."
      ],
      "metadata": {
        "id": "Ep0srL6QZ9nI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punto 4**"
      ],
      "metadata": {
        "id": "YKOmgSTcdDTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_nospam = df[df['label_num'] == 0]"
      ],
      "metadata": {
        "id": "VFBW-qjXaHRz"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_nospam.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wzlKwMYdNp_",
        "outputId": "bcc90cd5-50f2-4e98-8ea4-58cbc8c8fbc9"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3672, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "4g2psvNLdPvP"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "organizations = []\n",
        "for sentence in df_nospam['text']:\n",
        "    doc = nlp(sentence)\n",
        "    for token in doc:\n",
        "        if str(token.ent_type_) == 'ORG':\n",
        "          organizations.append(str(token))"
      ],
      "metadata": {
        "id": "zizAqMnndiY-"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "org = set(organizations)"
      ],
      "metadata": {
        "id": "y5PnHPpjCAmN"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(org)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9UlgspCCBtd",
        "outputId": "20be651d-2ce0-451a-8517-acc02aafff9f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'meters', 'shoreline', 'watches', 'evans', 'veselack', 'cayanosa', 'pacific', 'kim', 'ps', 'sql', 'lrc', 'aburrell', 'shut', 'north', 'steven', 'michelle', 'krishnaswamy', 'guerra', 'other', 'chokshi', 'carrabine', 'western', 'now', 'robin', 'investments', 'brooks', 'baptist', 'pec', 'paso', 'lebroc', 'aquila', 'coleman', 'mundt', 'white', 'errigo', 'sigmund', 'phillips', 'staff', 'dropbox', 'home', 'net', 'merchant', 'ceo', 'cec', 'schield', 'teco', 'milnthorp', 'refinery', 'ga', 'falcone', 'for', 'landman', 'sap', 'anne', 'espinoza', 'federal', 'list', 'advisors', 'p', 'stephanie', 'co', 'portland', 'cornshucker', 'east', 'panther', 'wal', 'morris', 'gore', 'corey', 'srm', 'america', 'states', '7', 'marron', 'and', 'lloyds', 'stephenson', 'lagrasta', 'jayson', 'thanls', '77002', 'congress', 'koenig', 'hillman', 'telephone', 'hobbs', 'yuma', 'ferries', '679847', 'luu', 'county', 'clynes', 'under', 'byron', 'kelly', 'future', 'lia', 'phibro', 'lockhart', 'bruce', 'jprejean', 'kemp', 'utilities', 'lauer', 'bp', 'sta', 'phx', 'independent', 'express', 'spring', 'financial', 'houston', '*', '9651', 'sid', 'gpa', 'myers', 'business', 'hilcorp', 'ibm', 'mark', 'richard', 'cfp', 'mills', 'wells', 'calgary', 'scada', 'mcdonald', 'cold', 'yahoo', 'tenn', '2002', 'herrera', 'distribution', 'pwcglobal', 'citigroup', 'shoes', 'exempt', 'gcp', 'puerto', 'ihii', 'southwest', 'jeni', 'hlavaty', 'devon', 'schedule', 'fri', 'valero', 'victor', 'ford', 'clint', 'urbanek', 'lilly', 'boas', 'prom', 'harvard', 'lisa', 'thais', 'info', 'trade', 'elektro', 'praxair', 'philippians', 'ltd', 'dress', 'jerriw', 'columbus', 'resources', 'hy', 'pioneer', 'lasuzzo', '/', 'capital', 'coast', 'asltom', 'gerard', 'specialist', 'mastercard', 'christian', '1', 'texaco', 'house', 'power', 'securities', 'company', '9672', 'camille', 'madeline', 'companies', 'as', 'mci', 'delta', 'alstom', 'nbc', '6040', 'airlines', 'mitchell', 'account', 'california', '12', 'aepin', 'ots', 'hensch', 'kilgore', 'jennings', 'representatives', 'aep', 'ngpa', 'reservations', 'tholt', 'syrup', 'eickenroht', 'internet', 'repairs', 'iv', 'ernst', 'syntroleum', 'elisabeth', 'alex', '28', 'carey', 'cardoso', 'saxet', 'charles', 'group', 'ets', 'cheryl', 'cn', 'sabrae', 'ferc', 'robinson', 'paul', 'americas', 'plus', 'utilicorp', 'gat', 'cynet', 'pasadena', 'gustafson', 'hotel', 'jayanta', 'schockling', 'ethyl', 'acton', 'helen', 'code', 'marsh', 'zamer', '11', 'plc', 'zukin', 'supreme', 'sharifi', '>', 'miller', 'llc', 'mason', 'cst', 'rose', 'nathan', '547201', 'newswires', 'mcgowan', 'prop', 'georgia', 'texas', '8', 'berkeland', 'corby', 'dfarmer', 'united', 'enron', 'enbridge', 'mips', 'exchange', 'point', 'wilkinson', 'bike', 'laynie', 'jackson', 'jason', 'fidelity', 'abc', 'nelson', 'w', 'coastal', 'journal', 'lateral', 'office', 'wiesepape', 'rabia', 'hill', 'systems', 'vuittonet', 'hpl', 'el', 'gpm', 'date', 'vastar', 'southern', 'bro', 'i', 'walia', 'order', 'at', 'bau', 'revision', 'edt', 'hughes', 'feb', 'phoenix', 'denton', 'espn', 'pefs', 'contract', 's', 'chase', 'central', 'katy', 'hsc', 'chevron', '30', 'mgi', 'electric', 'cavalier', 'meter', 'tenaska', 'anc', 'savings', 'balfour', 'non', 'aladdin', 'pdf', 'mar', 'tax', '93728', 'subject', 'merrill', 'lynda', 'meadowtree', 'gardens', 'mulholland', 'start', 'desktop', 'calc', 'kingston', 'esperanza', 'blue', 'usa', 'newton', 'asa', 'wilson', 'kafus', 'records', 'parker', 'dominique', 'duke', '9767', 'sutton', '9385', 'star', 'daren', '9826', 'clem', 'xiaojun', 'pennzoil', 'services', 'basics', 'god', 'holland', 'hol', 'south', 'painewebber', 'baskeball', 'syscom', 'roberts', 'sa', 'air', 'davis', 'antioch', 'rptg', 'e', 'mjkereluk', 'ngptrs', 'christina', 'construction', 'davies', 'connie', 'transmission', 'alan', 'treasury', 'origination', 'lasercomm', 'briana', 'zajac', 'u', 'alka', 'donna', 'committee', 'tammi', 'midcon', 'ranch', '27', 'wilmar', 'nicholie', 'lindholm', 'ermes', 'jared', 'bartz', '15600', 'flu', 'clickathome', 'transport', 'acosta', 'trustee', 'young', 'gop', 'invoice', 'lavorato', 'diet', 'environmental', 'cpl', 'pybus', 'quanta', 'cbs', 'daniel', '-', '591307', 'lawn', 'lincoln', 'deloitte', 'l', 'gcs', 'me', 'un', 'logistics', 'mart', 'archer', 'adam', 'path', 'flanagan', 'ewn', 'doesn', 'york', '9841', 'hou', 'unocal', 'university', 'edith', '834134', 'klussman', 'mineral', 'hurta', 'king', 'albrecht', 'kelley', 'choyce', 'beaumont', 'nyse', 'deboisblanc', '#', 'williams', 'schott', 'on', 'melinda', 'apr', 'villagomez', 'lim', 'macivor', 'lacy', 'fayett', 'station', 'end', 'buckley', 'fig', \"'\", 'hirl', 'kansas', '10', 'broadband', '04', 'smith', '9603', 'mtbe', 'crosstex', 'jim', 'juneman', 'jersey', '960', 'lamadrid', 'mcmills', 'd', 'spinaker', 'crzo', 'dewett', '02', 'jody', 'goodman', 'mjg', 'ellis', 'arco', 'crown', 'tennessee', '9739', 'vcf', 'pamela', 'law', 'carrizo', 'plant', 'ntp', 'kyle', 'attractions', 'adamik', 'regan', 'commonwealth', 'cnn', 'partnership', 'the', 'rate', 'juana', 'nov', 'times', 'tejon', 'bank', 'thompson', 'aiken', 'powder', 'graham', 'dreyfus', 'brent', '2001', 'address', 'inc', 'bless', 'hayden', 'jennifer', 'philadelphia', 'he', 'chewable', 'bagha', 'economics', '.', 'computer', 'franklin', 'hertz', 'richardson', 'return', 'hoong', 'resource', 'ebb', 'norton', 'rockefeller', 'ge', 'carthage', 'standards', 'methodist', 'althaus', 'speckels', 'thay', 'djfr', 'pat', 'marketing', 'ubs', 'joseph', 'missionary', 'westvaco', ';', 'go', 'hanz', 'walters', 'chosen', 'scott', 'lotus', 'land', 'truro', 'pierson', 'church', 'r', 'book', 'irs', 'fariba', 'katherine', 'harris', 'veronica', 'flow', 'jones', 'wgr', 'management', 'danex', 'munkres', 'miles', 'brad', 'spiegelhauer', 'eott', 'inter', 'tv', 'jewelry', 'berclair', 'pentagon', 'associate', 'jdf', 'resort', 'wesneske', 'commerce', 'mchiasson', 'cfgl', 'theresa', 'laura', 'dell', 'servers', 'army', 'cross', 'thompsonville', 'sordo', 'falcon', 'cc', 'jeanne', 'amanda', 'communications', 'louisiana', 'cocavessis', '05', 'legal', 'fbi', 'transportation', 'hetherington', 'spinnaker', 'debbie', 'hodge', 'whihc', 'warner', 'abramo', 'commission', 'senate', 'contact', 'omaha', 'arn', 'rsn', 'bell', 'send', 'asst', 'senior', 'charlotte', 'carbide', 'kinsey', 'lamay', 'morgan', 'tesoro', 'cox', 'control', 'lomak', 'teradyne', 'amoco', 'coe', 'deductions', 'oil', 'ena', 'gathering', 'channel', 'black', 'ocean', '1063', 'amit', 'sullivan', 'of', 'street', '0980308', 'industries', '(', 'hidden', 'once', 'fcc', 'johnson', 'complaint', 'cynthia', 'exploration', 'print', 'griffith', 'field', 'golf', '6', 'cindy', 'mecca', 'didn', 'texoma', 'zeman', 'reliability', '2', 'ygcy', 'desc', 'bridgeline', 'pollack', 'beaty', ')', 'gco', 'kevin', 'alamo', 'chemical', 'london', 'cooperative', 'ect', 'ext', 'hernandez', 'touche', 'od', 'carl', '679848', 'clu', 'abb', 'th', 'patti', 'healthcare', 'foon', ':', 'exxonmobil', 'volume', 'cdnow', 'julissa', 'methanol', '_', 'american', '@', 'lee', 'treasurer', 'lousis', 'v', 'sladana', 'n', 'basit', 'brown', '626231', 'barnett', 'sears', 'due', 'moore', 'notice', 'c', 'needs', 'noms', 'council', 'gulf', 'mother', 'jr', 'angela', 'greenery', 'burnell', 'corp', 'engineering', 'waha', 'milbank', 'funding', 'brenham', 'notify', 'denny', 'trading', 'msnbc', 'paralegal', 'june', 'baum', 'union', 'stg', 'aimee', 'aubrey', 'comair', 'coral', '0435', '4', '70120', 'george', 'conoco', '36', 'founder', 'enichem', 'nyu', 'lines', 'hinze', 'sukaly', 'triaminic', 'campon', 'costilla', 'donald', 'nick', 'hplc', '9794', 'ami', 'neuweiler', 'garden', 'rollup', 'opal', 'lonestar', 'called', 'apache', '18', 'edit', 'igs', 'chambers', 'dana', 'gmt', 'ec', 'healthgroup', 'lehman', '&', 'guadalupe', 'mscott', 'cinco', 'martinez', 'dynegy', 'anywhere', 'consulting', 'mcloughlin', 'superior', 'tnpc', 'stanley', 'justin', 'sherlyn', 'canada', 'mack', 'diaz', 'new', 'camden', 'ews', '$', 'megan', 'gepl', 'paladin', 'delivery', 'phc', 'burlington', 'rohm', 'morela', 'cooper', 'lyondell', 'can', 'handling', 'rick', 'couvillon', 'enw', 'to', 'kemper', 'mcgee', 'gpg', 'philip', '598934', 'less', 'actual', 'sterling', 'pai', 'country', 'tonya', 'reorg', 'raleigh', 'support', 'controls', 'palmer', 'limited', '35796', 'may', 'am', 'mendell', 'clarke', 'confirmation', 'fujitsu', '20000813', '9747', 'please', 'anderson', 'jaquet', 'sos', 'garrick', '3', 'reporting', 't', 'stuckey', 'venita', 'fox', 'national', 'pan', 'mmc', '20', 'position', 'nightly', 'walker', 'boeing', 'rico', 'moody', 'horton', '00', 'mei', 'salt', '4179', 'tobacco', 'allergy', 'walt', 'justice', 'john', 'marks', 'department', 'plg', 'jillian', 'energy', 'information', 'iferc', 'belco', 'college', 'niamh', 'samson', '1428', 'higginsville', 'anna', 'eastrans', 'cdp', 'emerald', '986884', 'see', 'jan', 'hl', 'pg', 'adrrf', 'art', 'driscoll', 'detroit', 'litigation', 'martin', 'cable', 'wagner', 'mails', 'loosley', 'campbell', 'jenuwine', 'xbr', 'tom', 'albemarle', 'jott', 'depaolis', 'microsoft', 'alexus', 'uunila', 'herod', 'desk', 'ebs', 'christopher', 'bulletin', 'petroleum', 'msn', '\\x01', 'apachi', 'janzen', 'a', 'supply', 'transco', 'truong', 'onshore', 'marlin', 'sec', 'lindley', 'darren', 'dev', 'citibank', 'reg', 'ohio', 'latson', 'al', 'mike', ',', 'board', 'sr', 'minerals', 'shanghai', 'football', 'porche', 'night', '\\n', 'pond', 'mccabe', 'dominic', 'h', 'coin', 'freud', 'vargas', 'expedia', 'hendrickson', 'nng', 'redelivery', 'florida', 'cowtrap', 'holmelin', 'mobil', 'development', 'intuit', 'o', 'scotland', 'jeanmard', 'templeton', 'searobin', 'elmendorf', 'hydorcarbons', 'centana', 'orsak', 'leila', 'harwell', 'dow', 'shell', 'bement', 'ward', 'et', 'corporation', 'gas', 'announcements', 'riley', 'transm', 'staab', 'mondragon', 'georganne', 'sony', 'smu', 'lloyd', 'reinhardt', 'coo', 'answers', 'pleo', 'rey', 'hr', '0', 'jo', 'q', 'nolan', 'shona', 'type', '5', 'oakville', 'forster', 'resume', 'ln', 'fitts', 'community', 'acutrim', '08', '5053', '555', 'foot', 'devries', 'darrel', 'rincon', '112', 'cummins', 'inland', 'st', 'bcc', 'bmc', 'hios', 'comtrex', 'ecf', 'gutschlag', 'president', 'williamson', 'julie', 'sinus', 'exxon', 'locker', 'midstream', '00534580', 'creek', 'court', 'mwongozi', 'wendy', 'effervescent', 'murray', 'operating', 'liz', 'rhone', '210722', 'payne', 'daigle', 'apollo', '1256', 'global', '\\\\', 'crook', 'kinder', 'robert', 'city', 'cavanaug', '989813', 'lone', 'cp', 'association', 'manhattan', 'wilcott', 'nasal', 'blockbuster', 'public', 'janet', 'atmic', 'louis', 'valley', 'medicine', 'kerr', 'nom', 'lunz', 'shannel', 'triaminicol', 'wukasch', 'pipline', 'general', 'edison', 'cough', 'fortin', 'leblanc', 'capitol', 'stock', '611178', 'cattle', 'nfl', 'collins', 'koch', 'seltzer', 'org', 'eb', 'ene', 'becken', 'dustin', 'plano', 'mrha', 'haas', 'baby', 'g', 'merat', 'tammy', 'odum', 'riegler', 'howard', 'india', 'reuters', 'first', 'bradley', 'deal', 'dodge', '2000', 'bienenstock', 'fda', 'buys', 'club', 'entergy', 'weldon', 'sheryl', 'service', 'international', '\\x12', '01', 'your', 'sanchez', 'pendergrass', 'txu', 'kravas', 'walter', 'esa', 'beau', 'mktg', 'bob', 'planning', 'news', '35', 'joann', 'inquirer', 'netco', 'karen', 'shipping', 'terminations', 'harder', 'fed', 'james', 'pipeline', 'schockmel', 'expirations', '069', '598958', 'helmerich', 'ag', '03', 'stacey', 'anal', 'nat', 'chris', 'brenda', 'shot', 'yates', 'stella', 'boyle', 'deere', 'concurrent', 'mccoy', 'oplc', 'finance', 'piper', 'gda', 'will', '012', 'buddy', 'ref', 'safety', 'm', 'ees', 'usb', 'effective', 'nomination', 'dnb', 'us'}\n"
          ]
        }
      ]
    }
  ]
}