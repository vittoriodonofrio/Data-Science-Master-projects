{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86dece0e-63bf-46a6-b39e-270ac61d39ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-03 08:34:14--  https://proai-datasets.s3.eu-west-3.amazonaws.com/wikipedia.csv\r\nResolving proai-datasets.s3.eu-west-3.amazonaws.com (proai-datasets.s3.eu-west-3.amazonaws.com)... 3.5.224.150, 52.95.154.94\r\nConnecting to proai-datasets.s3.eu-west-3.amazonaws.com (proai-datasets.s3.eu-west-3.amazonaws.com)|3.5.224.150|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 1003477941 (957M) [text/csv]\r\nSaving to: ‘wikipedia.csv.1’\r\n\r\n\rwikipedia.csv.1       0%[                    ]       0  --.-KB/s               \rwikipedia.csv.1       0%[                    ]  57.49K   214KB/s               \rwikipedia.csv.1       0%[                    ] 147.53K   274KB/s               \rwikipedia.csv.1       0%[                    ] 323.89K   401KB/s               \rwikipedia.csv.1       0%[                    ] 820.82K   763KB/s               \rwikipedia.csv.1       0%[                    ]   1.37M  1.02MB/s               \rwikipedia.csv.1       0%[                    ]   2.00M  1.24MB/s               \rwikipedia.csv.1       0%[                    ]   2.70M  1.43MB/s               \rwikipedia.csv.1       0%[                    ]   3.46M  1.61MB/s               \rwikipedia.csv.1       0%[                    ]   4.29M  1.78MB/s               \rwikipedia.csv.1       0%[                    ]   5.22M  1.95MB/s               \rwikipedia.csv.1       0%[                    ]   6.25M  2.11MB/s               \rwikipedia.csv.1       0%[                    ]   7.38M  2.29MB/s    eta 6m 55s \rwikipedia.csv.1       0%[                    ]   8.63M  2.47MB/s    eta 6m 55s \rwikipedia.csv.1       1%[                    ]   9.99M  2.66MB/s    eta 6m 55s \rwikipedia.csv.1       1%[                    ]  11.50M  2.85MB/s    eta 6m 55s \rwikipedia.csv.1       1%[                    ]  13.16M  3.09MB/s    eta 5m 5s  \rwikipedia.csv.1       1%[                    ]  14.97M  3.55MB/s    eta 5m 5s  \rwikipedia.csv.1       1%[                    ]  16.00M  3.81MB/s    eta 5m 5s  \rwikipedia.csv.1       1%[                    ]  18.10M  4.49MB/s    eta 5m 5s  \rwikipedia.csv.1       2%[                    ]  20.10M  4.95MB/s    eta 5m 5s  \rwikipedia.csv.1       2%[                    ]  21.69M  5.29MB/s    eta 3m 50s \rwikipedia.csv.1       2%[                    ]  23.89M  5.67MB/s    eta 3m 50s \rwikipedia.csv.1       2%[                    ]  25.86M  6.13MB/s    eta 3m 50s \rwikipedia.csv.1       2%[                    ]  28.39M  6.82MB/s    eta 3m 50s \rwikipedia.csv.1       3%[                    ]  30.66M  7.36MB/s    eta 3m 50s \rwikipedia.csv.1       3%[                    ]  33.53M  7.87MB/s    eta 2m 55s \rwikipedia.csv.1       3%[                    ]  35.96M  8.25MB/s    eta 2m 55s \rwikipedia.csv.1       4%[                    ]  38.79M  8.93MB/s    eta 2m 55s \rwikipedia.csv.1       4%[                    ]  41.24M  9.51MB/s    eta 2m 55s \rwikipedia.csv.1       4%[                    ]  44.02M  10.0MB/s    eta 2m 55s \rwikipedia.csv.1       4%[                    ]  46.53M  10.3MB/s    eta 2m 24s \rwikipedia.csv.1       5%[>                   ]  49.32M  10.7MB/s    eta 2m 24s \rwikipedia.csv.1       5%[>                   ]  51.80M  11.1MB/s    eta 2m 24s \rwikipedia.csv.1       5%[>                   ]  54.50M  11.6MB/s    eta 2m 24s \rwikipedia.csv.1       5%[>                   ]  57.10M  11.8MB/s    eta 2m 24s \rwikipedia.csv.1       6%[>                   ]  59.78M  12.2MB/s    eta 2m 5s  \rwikipedia.csv.1       6%[>                   ]  62.41M  12.5MB/s    eta 2m 5s  \rwikipedia.csv.1       6%[>                   ]  65.10M  12.6MB/s    eta 2m 5s  \rwikipedia.csv.1       7%[>                   ]  67.77M  12.9MB/s    eta 2m 5s  \rwikipedia.csv.1       7%[>                   ]  70.42M  13.2MB/s    eta 2m 5s  \rwikipedia.csv.1       7%[>                   ]  73.16M  13.1MB/s    eta 1m 53s \rwikipedia.csv.1       7%[>                   ]  75.86M  13.3MB/s    eta 1m 53s \rwikipedia.csv.1       8%[>                   ]  78.63M  13.3MB/s    eta 1m 53s \rwikipedia.csv.1       8%[>                   ]  81.13M  13.1MB/s    eta 1m 53s \rwikipedia.csv.1       8%[>                   ]  84.10M  13.1MB/s    eta 1m 53s \rwikipedia.csv.1       9%[>                   ]  86.60M  13.2MB/s    eta 1m 44s \rwikipedia.csv.1       9%[>                   ]  89.60M  13.3MB/s    eta 1m 44s \rwikipedia.csv.1       9%[>                   ]  92.11M  13.1MB/s    eta 1m 44s \rwikipedia.csv.1       9%[>                   ]  95.14M  13.4MB/s    eta 1m 44s \rwikipedia.csv.1      10%[=>                  ]  97.60M  13.5MB/s    eta 1m 44s \rwikipedia.csv.1      10%[=>                  ] 100.75M  13.1MB/s    eta 97s    \rwikipedia.csv.1      10%[=>                  ] 104.50M  13.6MB/s    eta 97s    \rwikipedia.csv.1      11%[=>                  ] 106.41M  13.4MB/s    eta 97s    \rwikipedia.csv.1      11%[=>                  ] 110.17M  13.9MB/s    eta 97s    \rwikipedia.csv.1      11%[=>                  ] 112.10M  13.4MB/s    eta 97s    \rwikipedia.csv.1      12%[=>                  ] 115.85M  13.6MB/s    eta 90s    \rwikipedia.csv.1      12%[=>                  ] 118.17M  13.9MB/s    eta 90s    \rwikipedia.csv.1      12%[=>                  ] 121.49M  13.9MB/s    eta 90s    \rwikipedia.csv.1      12%[=>                  ] 123.88M  13.6MB/s    eta 90s    \rwikipedia.csv.1      13%[=>                  ] 127.27M  13.8MB/s    eta 90s    \rwikipedia.csv.1      13%[=>                  ] 129.80M  14.0MB/s    eta 86s    \rwikipedia.csv.1      13%[=>                  ] 133.16M  14.0MB/s    eta 86s    \rwikipedia.csv.1      14%[=>                  ] 135.61M  13.8MB/s    eta 86s    \rwikipedia.csv.1      14%[=>                  ] 139.14M  14.2MB/s    eta 86s    \rwikipedia.csv.1      14%[=>                  ] 141.64M  14.3MB/s    eta 86s    \rwikipedia.csv.1      15%[==>                 ] 145.19M  14.1MB/s    eta 81s    \rwikipedia.csv.1      15%[==>                 ] 149.19M  14.6MB/s    eta 81s    \rwikipedia.csv.1      15%[==>                 ] 151.66M  14.6MB/s    eta 81s    \rwikipedia.csv.1      16%[==>                 ] 155.22M  14.5MB/s    eta 81s    \rwikipedia.csv.1      16%[==>                 ] 157.69M  14.3MB/s    eta 81s    \rwikipedia.csv.1      16%[==>                 ] 161.27M  14.7MB/s    eta 77s    \rwikipedia.csv.1      17%[==>                 ] 163.75M  14.7MB/s    eta 77s    \rwikipedia.csv.1      17%[==>                 ] 167.36M  15.0MB/s    eta 77s    \rwikipedia.csv.1      17%[==>                 ] 169.85M  14.7MB/s    eta 77s    \rwikipedia.csv.1      18%[==>                 ] 173.49M  14.9MB/s    eta 77s    \rwikipedia.csv.1      18%[==>                 ] 175.97M  15.0MB/s    eta 73s    \rwikipedia.csv.1      18%[==>                 ] 179.66M  15.2MB/s    eta 73s    \rwikipedia.csv.1      19%[==>                 ] 182.15M  14.7MB/s    eta 73s    \rwikipedia.csv.1      19%[==>                 ] 185.83M  15.1MB/s    eta 73s    \rwikipedia.csv.1      19%[==>                 ] 188.28M  15.1MB/s    eta 73s    \rwikipedia.csv.1      20%[===>                ] 192.02M  15.4MB/s    eta 70s    \rwikipedia.csv.1      20%[===>                ] 194.27M  14.8MB/s    eta 70s    \rwikipedia.csv.1      20%[===>                ] 198.38M  15.3MB/s    eta 70s    \rwikipedia.csv.1      20%[===>                ] 200.71M  15.3MB/s    eta 70s    \rwikipedia.csv.1      21%[===>                ] 205.00M  15.5MB/s    eta 70s    \rwikipedia.csv.1      21%[===>                ] 207.33M  15.1MB/s    eta 67s    \rwikipedia.csv.1      22%[===>                ] 211.71M  15.5MB/s    eta 67s    \rwikipedia.csv.1      22%[===>                ] 214.08M  15.4MB/s    eta 67s    \rwikipedia.csv.1      22%[===>                ] 218.50M  16.1MB/s    eta 67s    \rwikipedia.csv.1      23%[===>                ] 220.94M  15.7MB/s    eta 67s    \rwikipedia.csv.1      23%[===>                ] 225.36M  15.9MB/s    eta 64s    \rwikipedia.csv.1      23%[===>                ] 227.86M  15.9MB/s    eta 64s    \rwikipedia.csv.1      24%[===>                ] 232.25M  16.5MB/s    eta 64s    \rwikipedia.csv.1      24%[===>                ] 234.77M  16.4MB/s    eta 64s    \rwikipedia.csv.1      24%[===>                ] 239.17M  16.4MB/s    eta 64s    \rwikipedia.csv.1      25%[====>               ] 241.69M  16.4MB/s    eta 61s    \rwikipedia.csv.1      25%[====>               ] 246.10M  17.0MB/s    eta 61s    \rwikipedia.csv.1      25%[====>               ] 248.63M  16.8MB/s    eta 61s    \rwikipedia.csv.1      26%[====>               ] 253.03M  16.8MB/s    eta 61s    \rwikipedia.csv.1      26%[====>               ] 255.58M  16.6MB/s    eta 61s    \rwikipedia.csv.1      27%[====>               ] 259.99M  17.0MB/s    eta 58s    \rwikipedia.csv.1      27%[====>               ] 262.58M  17.1MB/s    eta 58s    \rwikipedia.csv.1      27%[====>               ] 267.03M  17.3MB/s    eta 58s    \rwikipedia.csv.1      28%[====>               ] 269.63M  16.8MB/s    eta 58s    \rwikipedia.csv.1      28%[====>               ] 274.07M  17.2MB/s    eta 58s    \rwikipedia.csv.1      28%[====>               ] 276.67M  17.0MB/s    eta 56s    \rwikipedia.csv.1      29%[====>               ] 281.14M  17.5MB/s    eta 56s    \rwikipedia.csv.1      29%[====>               ] 283.74M  17.2MB/s    eta 56s    \rwikipedia.csv.1      30%[=====>              ] 288.24M  17.3MB/s    eta 56s    \rwikipedia.csv.1      30%[=====>              ] 290.94M  17.0MB/s    eta 56s    \rwikipedia.csv.1      30%[=====>              ] 295.46M  17.5MB/s    eta 53s    \rwikipedia.csv.1      31%[=====>              ] 298.19M  17.4MB/s    eta 53s    \rwikipedia.csv.1      31%[=====>              ] 302.75M  17.9MB/s    eta 53s    \rwikipedia.csv.1      31%[=====>              ] 305.50M  17.6MB/s    eta 53s    \rwikipedia.csv.1      32%[=====>              ] 310.11M  17.6MB/s    eta 53s    \rwikipedia.csv.1      32%[=====>              ] 312.88M  17.5MB/s    eta 51s    \rwikipedia.csv.1      33%[=====>              ] 317.50M  17.9MB/s    eta 51s    \rwikipedia.csv.1      33%[=====>              ] 320.25M  17.9MB/s    eta 51s    \rwikipedia.csv.1      33%[=====>              ] 324.91M  18.2MB/s    eta 51s    \rwikipedia.csv.1      34%[=====>              ] 327.71M  17.6MB/s    eta 51s    \rwikipedia.csv.1      34%[=====>              ] 332.41M  18.1MB/s    eta 48s    \rwikipedia.csv.1      35%[======>             ] 335.21M  17.9MB/s    eta 48s    \rwikipedia.csv.1      35%[======>             ] 339.97M  18.5MB/s    eta 48s    \rwikipedia.csv.1      35%[======>             ] 342.75M  18.3MB/s    eta 48s    \rwikipedia.csv.1      36%[======>             ] 347.53M  18.3MB/s    eta 48s    \rwikipedia.csv.1      36%[======>             ] 350.33M  18.1MB/s    eta 46s    \rwikipedia.csv.1      37%[======>             ] 355.19M  18.5MB/s    eta 46s    \rwikipedia.csv.1      37%[======>             ] 357.98M  18.5MB/s    eta 46s    \rwikipedia.csv.1      37%[======>             ] 362.89M  18.9MB/s    eta 46s    \rwikipedia.csv.1      38%[======>             ] 365.61M  18.6MB/s    eta 46s    \rwikipedia.csv.1      38%[======>             ] 370.71M  18.7MB/s    eta 44s    \rwikipedia.csv.1      39%[======>             ] 373.41M  18.5MB/s    eta 44s    \rwikipedia.csv.1      39%[======>             ] 378.67M  19.1MB/s    eta 44s    \rwikipedia.csv.1      39%[======>             ] 381.41M  18.9MB/s    eta 44s    \rwikipedia.csv.1      40%[=======>            ] 386.57M  19.4MB/s    eta 44s    \rwikipedia.csv.1      40%[=======>            ] 389.74M  18.8MB/s    eta 42s    \rwikipedia.csv.1      41%[=======>            ] 394.67M  19.1MB/s    eta 42s    \rwikipedia.csv.1      41%[=======>            ] 398.00M  19.0MB/s    eta 42s    \rwikipedia.csv.1      42%[=======>            ] 402.80M  19.6MB/s    eta 42s    \rwikipedia.csv.1      42%[=======>            ] 405.99M  19.4MB/s    eta 42s    \rwikipedia.csv.1      42%[=======>            ] 411.03M  19.8MB/s    eta 39s    \rwikipedia.csv.1      43%[=======>            ] 414.05M  19.2MB/s    eta 39s    \rwikipedia.csv.1      43%[=======>            ] 418.88M  19.7MB/s    eta 39s    \rwikipedia.csv.1      44%[=======>            ] 422.16M  19.7MB/s    eta 39s    \rwikipedia.csv.1      44%[=======>            ] 426.80M  20.0MB/s    eta 39s    \rwikipedia.csv.1      44%[=======>            ] 430.24M  19.8MB/s    eta 38s    \rwikipedia.csv.1      45%[========>           ] 434.69M  19.6MB/s    eta 38s    \rwikipedia.csv.1      45%[========>           ] 438.39M  19.5MB/s    eta 38s    \rwikipedia.csv.1      46%[========>           ] 442.25M  19.7MB/s    eta 38s    \rwikipedia.csv.1      46%[========>           ] 446.52M  20.0MB/s    eta 38s    \rwikipedia.csv.1      47%[========>           ] 450.61M  19.7MB/s    eta 36s    \rwikipedia.csv.1      47%[========>           ] 455.78M  19.8MB/s    eta 36s    \rwikipedia.csv.1      48%[========>           ] 460.16M  20.0MB/s    eta 36s    \rwikipedia.csv.1      48%[========>           ] 464.24M  19.8MB/s    eta 36s    \rwikipedia.csv.1      49%[========>           ] 469.39M  19.7MB/s    eta 36s    \rwikipedia.csv.1      49%[========>           ] 473.17M  19.8MB/s    eta 34s    \rwikipedia.csv.1      49%[========>           ] 477.44M  20.1MB/s    eta 34s    \rwikipedia.csv.1      50%[=========>          ] 481.75M  20.1MB/s    eta 34s    \rwikipedia.csv.1      50%[=========>          ] 486.16M  19.7MB/s    eta 34s    \rwikipedia.csv.1      51%[=========>          ] 490.92M  20.0MB/s    eta 34s    \rwikipedia.csv.1      51%[=========>          ] 495.38M  20.1MB/s    eta 32s    \rwikipedia.csv.1      52%[=========>          ] 499.71M  19.8MB/s    eta 32s    \rwikipedia.csv.1      52%[=========>          ] 503.75M  19.6MB/s    eta 32s    \rwikipedia.csv.1      53%[=========>          ] 509.25M  19.9MB/s    eta 32s    \rwikipedia.csv.1      53%[=========>          ] 512.46M  19.6MB/s    eta 32s    \rwikipedia.csv.1      54%[=========>          ] 517.27M  20.2MB/s    eta 30s    \rwikipedia.csv.1      54%[=========>          ] 520.44M  19.7MB/s    eta 30s    \rwikipedia.csv.1      54%[=========>          ] 525.28M  19.4MB/s    eta 30s    \rwikipedia.csv.1      55%[==========>         ] 530.86M  19.8MB/s    eta 30s    \rwikipedia.csv.1      56%[==========>         ] 536.49M  19.7MB/s    eta 28s    \rwikipedia.csv.1      56%[==========>         ] 541.74M  19.8MB/s    eta 28s    \rwikipedia.csv.1      56%[==========>         ] 544.74M  19.9MB/s    eta 28s    \rwikipedia.csv.1      57%[==========>         ] 549.77M  20.0MB/s    eta 28s    \rwikipedia.csv.1      57%[==========>         ] 553.14M  19.6MB/s    eta 28s    \rwikipedia.csv.1      58%[==========>         ] 558.13M  20.1MB/s    eta 26s    \rwikipedia.csv.1      58%[==========>         ] 561.46M  19.8MB/s    eta 26s    \rwikipedia.csv.1      59%[==========>         ] 566.55M  20.2MB/s    eta 26s    \rwikipedia.csv.1      59%[==========>         ] 569.61M  20.0MB/s    eta 26s    \rwikipedia.csv.1      60%[===========>        ] 574.99M  20.2MB/s    eta 26s    \rwikipedia.csv.1      60%[===========>        ] 578.08M  20.5MB/s    eta 25s    \rwikipedia.csv.1      60%[===========>        ] 583.46M  20.6MB/s    eta 25s    \rwikipedia.csv.1      61%[===========>        ] 586.53M  20.1MB/s    eta 25s    \rwikipedia.csv.1      61%[===========>        ] 592.02M  20.2MB/s    eta 25s    \rwikipedia.csv.1      62%[===========>        ] 597.71M  20.2MB/s    eta 25s    \rwikipedia.csv.1      63%[===========>        ] 603.27M  20.4MB/s    eta 23s    \rwikipedia.csv.1      63%[===========>        ] 608.71M  20.2MB/s    eta 23s    \rwikipedia.csv.1      64%[===========>        ] 614.27M  20.6MB/s    eta 23s    \rwikipedia.csv.1      64%[===========>        ] 619.52M  20.3MB/s    eta 23s    \rwikipedia.csv.1      65%[============>       ] 625.10M  20.2MB/s    eta 21s    \rwikipedia.csv.1      65%[============>       ] 630.33M  20.2MB/s    eta 21s    \rwikipedia.csv.1      66%[============>       ] 635.71M  20.8MB/s    eta 21s    \rwikipedia.csv.1      66%[============>       ] 638.75M  20.4MB/s    eta 21s    \rwikipedia.csv.1      67%[============>       ] 644.07M  20.6MB/s    eta 21s    \rwikipedia.csv.1      67%[============>       ] 646.85M  20.1MB/s    eta 20s    \rwikipedia.csv.1      68%[============>       ] 652.38M  20.8MB/s    eta 20s    \rwikipedia.csv.1      68%[============>       ] 655.00M  20.2MB/s    eta 20s    \rwikipedia.csv.1      69%[============>       ] 660.74M  20.2MB/s    eta 20s    \rwikipedia.csv.1      69%[============>       ] 666.46M  20.5MB/s    eta 20s    \rwikipedia.csv.1      70%[=============>      ] 671.88M  20.2MB/s    eta 18s    \rwikipedia.csv.1      70%[=============>      ] 677.55M  20.4MB/s    eta 18s    \rwikipedia.csv.1      71%[=============>      ] 682.85M  20.2MB/s    eta 18s    \rwikipedia.csv.1      71%[=============>      ] 687.97M  20.3MB/s    eta 18s    \rwikipedia.csv.1      72%[=============>      ] 693.66M  20.2MB/s    eta 17s    \rwikipedia.csv.1      73%[=============>      ] 699.46M  20.5MB/s    eta 17s    \rwikipedia.csv.1      73%[=============>      ] 704.94M  20.4MB/s    eta 17s    \rwikipedia.csv.1      74%[=============>      ] 709.99M  20.2MB/s    eta 17s    \rwikipedia.csv.1      74%[=============>      ] 715.53M  20.1MB/s    eta 15s    \rwikipedia.csv.1      75%[==============>     ] 721.22M  20.2MB/s    eta 15s    \rwikipedia.csv.1      75%[==============>     ] 726.80M  20.2MB/s    eta 15s    \rwikipedia.csv.1      76%[==============>     ] 731.49M  20.1MB/s    eta 15s    \rwikipedia.csv.1      77%[==============>     ] 736.89M  20.0MB/s    eta 14s    \rwikipedia.csv.1      77%[==============>     ] 742.57M  20.0MB/s    eta 14s    \rwikipedia.csv.1      78%[==============>     ] 748.30M  20.1MB/s    eta 14s    \rwikipedia.csv.1      78%[==============>     ] 753.27M  19.9MB/s    eta 14s    \rwikipedia.csv.1      79%[==============>     ] 759.17M  20.0MB/s    eta 12s    \rwikipedia.csv.1      79%[==============>     ] 765.07M  20.3MB/s    eta 12s    \rwikipedia.csv.1      80%[===============>    ] 770.11M  19.9MB/s    eta 12s    \rwikipedia.csv.1      81%[===============>    ] 775.19M  20.0MB/s    eta 12s    \rwikipedia.csv.1      81%[===============>    ] 780.97M  19.9MB/s    eta 11s    \rwikipedia.csv.1      82%[===============>    ] 786.74M  20.1MB/s    eta 11s    \rwikipedia.csv.1      82%[===============>    ] 792.57M  20.1MB/s    eta 11s    \rwikipedia.csv.1      83%[===============>    ] 797.92M  20.3MB/s    eta 11s    \rwikipedia.csv.1      83%[===============>    ] 803.36M  20.5MB/s    eta 9s     \rwikipedia.csv.1      84%[===============>    ] 809.17M  20.3MB/s    eta 9s     \rwikipedia.csv.1      85%[================>   ] 814.42M  20.2MB/s    eta 9s     \rwikipedia.csv.1      85%[================>   ] 820.27M  20.6MB/s    eta 9s     \rwikipedia.csv.1      86%[================>   ] 826.17M  20.4MB/s    eta 8s     \rwikipedia.csv.1      86%[================>   ] 831.36M  20.7MB/s    eta 8s     \rwikipedia.csv.1      87%[================>   ] 837.17M  20.5MB/s    eta 8s     \rwikipedia.csv.1      88%[================>   ] 842.67M  20.5MB/s    eta 8s     \rwikipedia.csv.1      88%[================>   ] 847.49M  20.3MB/s    eta 7s     \rwikipedia.csv.1      89%[================>   ] 853.38M  20.4MB/s    eta 7s     \rwikipedia.csv.1      89%[================>   ] 858.74M  20.2MB/s    eta 7s     \rwikipedia.csv.1      90%[=================>  ] 864.50M  20.5MB/s    eta 7s     \rwikipedia.csv.1      90%[=================>  ] 869.96M  20.5MB/s    eta 5s     \rwikipedia.csv.1      91%[=================>  ] 875.28M  20.6MB/s    eta 5s     \rwikipedia.csv.1      92%[=================>  ] 880.80M  20.4MB/s    eta 5s     \rwikipedia.csv.1      92%[=================>  ] 886.67M  20.7MB/s    eta 5s     \rwikipedia.csv.1      93%[=================>  ] 891.82M  20.3MB/s    eta 4s     \rwikipedia.csv.1      93%[=================>  ] 897.25M  20.1MB/s    eta 4s     \rwikipedia.csv.1      94%[=================>  ] 902.77M  20.4MB/s    eta 4s     \rwikipedia.csv.1      94%[=================>  ] 908.52M  20.3MB/s    eta 4s     \rwikipedia.csv.1      95%[==================> ] 913.50M  20.0MB/s    eta 3s     \rwikipedia.csv.1      96%[==================> ] 919.41M  20.3MB/s    eta 3s     \rwikipedia.csv.1      96%[==================> ] 924.83M  20.4MB/s    eta 3s     \rwikipedia.csv.1      97%[==================> ] 930.78M  20.4MB/s    eta 3s     \rwikipedia.csv.1      97%[==================> ] 936.33M  20.9MB/s    eta 1s     \rwikipedia.csv.1      98%[==================> ] 939.28M  20.3MB/s    eta 1s     \rwikipedia.csv.1      98%[==================> ] 944.67M  20.7MB/s    eta 1s     \rwikipedia.csv.1      99%[==================> ] 947.66M  20.6MB/s    eta 1s     \rwikipedia.csv.1      99%[==================> ] 952.77M  20.9MB/s    eta 1s     \rwikipedia.csv.1      99%[==================> ] 956.02M  20.7MB/s    eta 0s     \rwikipedia.csv.1     100%[===================>] 956.99M  20.5MB/s    in 57s     \r\n\r\n2024-05-03 08:35:11 (16.9 MB/s) - ‘wikipedia.csv.1’ saved [1003477941/1003477941]\r\n\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://proai-datasets.s3.eu-west-3.amazonaws.com/wikipedia.csv\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('/databricks/driver/wikipedia.csv')\n",
    "spark_df = spark.createDataFrame(dataset)\n",
    "spark_df = spark_df.drop(\"Unnamed: 0\")\n",
    "#spark_df.write.saveAsTable(\"wikipedia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88f3dc56-e663-4c7a-8d67-5128fc5dadb7",
     "showTitle": true,
     "title": "Punto 1)"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faee95bf-3b11-424d-a655-f51184048876",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---------+---------+\n|title|summary|documents|categoria|\n+-----+-------+---------+---------+\n|    0|    928|      928|        0|\n+-----+-------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "spark_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in spark_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bffea62-14dd-43ad-ac5f-1b78c8376013",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Si può notare che ci sono 928 articoli che hanno le colonne \"summary\" e \"documents\" vuote, si possono pertanto rimuovere dal dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "397825b0-d85d-40a7-be0d-e3ff4fb780b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark_df.na.drop(subset=[\"documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b678b18-cb71-4b6e-ad2a-373e15675f1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------+\n|  categoria|conteggio per categoria|\n+-----------+-----------------------+\n|  economics|                  10110|\n|   politics|                  11358|\n|    culture|                  10155|\n|    science|                  10166|\n|     sports|                  10066|\n|     energy|                  10033|\n|    finance|                   9863|\n| humanities|                  10116|\n|       pets|                  10016|\n|      trade|                  10064|\n| technology|                  10082|\n|  transport|                  10111|\n|   medicine|                  10015|\n|engineering|                  10219|\n|   research|                   9930|\n+-----------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "count = df.groupBy(\"categoria\").agg(F.count(\"*\").alias(\"conteggio per categoria\"))\n",
    "count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "521e7c32-31aa-4375-92fd-ede022c5841c",
     "showTitle": true,
     "title": "Punto 2"
    }
   },
   "source": [
    "Parte documents) \n",
    "\n",
    "In questa prima parte calcoliamo il numero medio di parole utilizzate in un articolo per categoria. Procediamo innanzitutto con la rimozione della punteggiatura e con la tokenizzazione; poi contiamo il numero di parole presenti in ogni articolo tramite la funzione size.\n",
    "A questo punto creiamo due tabelle per la somma e il numero di articoli per categoria; uniamo le due tabelle con una inner join e troviamo la media per categoria come rapporto tra somma e conteggio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f628a55e-1429-4249-a5fd-51cabc70318d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "punteggiatura_regex = \"[^\\w\\s]\"\n",
    "\n",
    "df = df.withColumn(\"documents\", F.regexp_replace(F.col(\"documents\"), punteggiatura_regex, \"\"))\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"documents\", outputCol=\"words\")\n",
    "\n",
    "\n",
    "tokenized_df = tokenizer.transform(df)\n",
    "\n",
    "\n",
    "tokenized_df = tokenized_df.withColumn(\"words\", F.array_remove(F.col(\"words\"), \"\"))\n",
    "\n",
    "\n",
    "word_count_df = tokenized_df.withColumn(\"word_count\", F.size(\"words\"))\n",
    "\n",
    "\n",
    "word_count_df = word_count_df.na.drop(subset=[\"documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4813c73-888d-4037-9845-201252f829fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------------+\n|  categoria|media parole articolo per categoria|\n+-----------+-----------------------------------+\n|  economics|                  979.1021760633037|\n|   politics|                   1512.54146856841|\n|    culture|                  634.5190898573081|\n|    science|                 1854.7100429855411|\n|     sports|                  598.8125744934446|\n|     energy|                  583.7126219390802|\n|    finance|                  1847.064487545535|\n| humanities|                 1042.6440015631106|\n|       pets|                 420.22857427915795|\n|      trade|                  638.5128128724672|\n| technology|                  884.4902426944032|\n|  transport|                  625.4437314906219|\n|   medicine|                   783.616514489877|\n|engineering|                  721.5844422700587|\n|   research|                  691.1322108199661|\n+-----------+-----------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "sum_word_per_category = word_count_df.groupBy(\"categoria\").agg(F.sum(\"word_count\").alias(\"sum_words_per_category\"))\n",
    "\n",
    "count_article_per_category = spark_df.groupBy(\"categoria\").agg(F.count(\"*\").alias(\"count_article_per_category\"))\n",
    "\n",
    "res = sum_word_per_category.join(count_article_per_category,  sum_word_per_category[\"categoria\"] == count_article_per_category[\"categoria\"], \"inner\" )\n",
    "\n",
    "res = res.withColumn( \"media parole articolo per categoria\" , F.col(\"sum_words_per_category\")/ F.col(\"count_article_per_category\") )\n",
    "\n",
    "res = res.select(sum_word_per_category[\"categoria\"] , \"media parole articolo per categoria\" )\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58588718-635c-4373-a7c1-fc79a6e3fec9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Parte summary)\n",
    "\n",
    "Ripetiamo gli stessi ragionamenti sulla colonna summary per ottenere la media di parole di un riassunto per ogni categoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca6f32b-ecbc-423c-8b95-8e505a7dcd50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "punteggiatura_regex = \"[^\\w\\s]\"\n",
    "\n",
    "df_summary = df.withColumn(\"summary\", F.regexp_replace(F.col(\"summary\"), punteggiatura_regex, \"\"))\n",
    "\n",
    "\n",
    "tokenizer_summary = Tokenizer(inputCol=\"summary\", outputCol=\"words\")\n",
    "\n",
    "\n",
    "tokenized_df_summary = tokenizer_summary.transform(df_summary)\n",
    "\n",
    "\n",
    "tokenized_df_summary = tokenized_df_summary.withColumn(\"words\", F.array_remove(F.col(\"words\"), \"\"))\n",
    "\n",
    "\n",
    "word_count_df_summary = tokenized_df_summary.withColumn(\"word_count\", F.size(\"words\"))\n",
    "\n",
    "\n",
    "word_count_df_summary = word_count_df_summary.na.drop(subset=[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c2d30cc-9060-401c-91dd-08cd7aab3dfd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------------------+\n|  categoria|media parole riassunto per categoria|\n+-----------+------------------------------------+\n|  economics|                  111.53135509396637|\n|   politics|                    177.117362211657|\n|    culture|                   99.99045507134593|\n|    science|                  104.67497069167644|\n|     sports|                   80.25784664282877|\n|     energy|                   96.41319928329683|\n|    finance|                  110.85290932361917|\n| humanities|                   85.59632669011333|\n|       pets|                   72.15594133492966|\n|      trade|                  107.62604290822408|\n| technology|                  103.19039128281328|\n|  transport|                   85.21510365251727|\n|   medicine|                   98.56331877729258|\n|engineering|                   88.44422700587084|\n|   research|                   90.37561024210422|\n+-----------+------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "sum_word_per_category_summary = word_count_df_summary.groupBy(\"categoria\").agg(F.sum(\"word_count\").alias(\"sum_words_per_category\"))\n",
    "\n",
    "res_summary = sum_word_per_category_summary.join(count_article_per_category,  sum_word_per_category_summary[\"categoria\"] == count_article_per_category[\"categoria\"], \"inner\" )\n",
    "\n",
    "res_summary = res_summary.withColumn( \"media parole riassunto per categoria\" , F.col(\"sum_words_per_category\")/ F.col(\"count_article_per_category\") )\n",
    "\n",
    "res_summary = res_summary.select(sum_word_per_category[\"categoria\"] , \"media parole riassunto per categoria\" )\n",
    "\n",
    "res_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65bd734d-a733-4b37-8deb-3c373f2b84ed",
     "showTitle": true,
     "title": "Punti 3 e 4)"
    }
   },
   "source": [
    "Sfruttando la colonna di conteggio presente nel dataset \"word_count_df\", si possono trovare massimo e minimo numero di parole di un articolo per categoria con una groupby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "997c2309-a14b-4bf8-8278-fca83b4e4afc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------+\n|  categoria|max_words_per_category|\n+-----------+----------------------+\n|  economics|                 23833|\n|   politics|                 20112|\n|    culture|                 15446|\n|    science|                 29369|\n|     sports|                 19215|\n|     energy|                 23201|\n|    finance|                 33442|\n| humanities|                 23186|\n|       pets|                 13217|\n|      trade|                 19264|\n| technology|                 18123|\n|  transport|                 22117|\n|   medicine|                 18412|\n|engineering|                 11837|\n|   research|                 27211|\n+-----------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "result_max = word_count_df.groupBy(\"categoria\").agg(F.max(\"word_count\").alias(\"max_words_per_category\"))\n",
    "result_max.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "793fdf15-9f29-4ce0-92ff-f222730685a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------+\n|  categoria|min_words_per_category|\n+-----------+----------------------+\n|  economics|                     8|\n|   politics|                     9|\n|    culture|                     9|\n|    science|                    13|\n|     sports|                    13|\n|     energy|                     9|\n|    finance|                     1|\n| humanities|                     7|\n|       pets|                    10|\n|      trade|                    14|\n| technology|                     2|\n|  transport|                     6|\n|   medicine|                    10|\n|engineering|                     8|\n|   research|                    15|\n+-----------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "result_min = word_count_df.groupBy(\"categoria\").agg(F.min(\"word_count\").alias(\"min_words_per_category\"))\n",
    "result_min.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d1be6aa-7d13-40bb-b258-fa1a36091a3c",
     "showTitle": true,
     "title": "Punto 5)"
    }
   },
   "source": [
    "Partendo dalla tabella \"tokenized_df\" ottenuta al punto 2, si rimuovono innanzitutto le stopwords, che saranno poco indicative di una categoria. Poi viene creato un modello countvectorizer e applicato al dataframe \"filtered_df\" per tenere traccia del conteggio delle parole presenti nella colonna \"filtered_words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61f0f781-1107-4da6-953b-7ebd7a088da4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_df = remover.transform(tokenized_df)\n",
    "\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"features\")\n",
    "cv_model = cv.fit(filtered_df)\n",
    "count_vectorized_df = cv_model.transform(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fb6e116-06e5-439e-bfbb-403100df1501",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A questo punto per trovare le parole più rappresentative per categoria, a partire dal vocabolario fornito dal count vectorizer possiamo trovare le 10 parole più frequenti (per questo viene implementata la funzione getTopWordContainer). Successivamente, si calcola la media delle features per categoria nel dataframe \"count_vectorized_df\" e quindi si usa la funzione definita prima per trovare le parole principali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8e5455a-8af8-4c0d-b762-3f6f0538278a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "def getTopWordContainer(v):\n",
    "    def getTopWord(vector):\n",
    "        vectorConverted = vector.toArray().tolist()\n",
    "        listSortedDesc= [i[0] for i in sorted(enumerate(vectorConverted), key=lambda x:x[1])][-10:][::-1]\n",
    "        return [v[j] for j in listSortedDesc]\n",
    "    return getTopWord\n",
    "\n",
    "getTopWordInit = getTopWordContainer(cv_model.vocabulary)\n",
    "getTopWord_udf = udf(getTopWordInit, ArrayType(StringType()))\n",
    "\n",
    "top = count_vectorized_df.groupBy(\"categoria\").agg(Summarizer.mean(F.col(\"features\")).alias(\"means\")) \\\n",
    "    .withColumn(\"topWord\", getTopWord_udf(F.col('means'))) \\\n",
    "    .select(\"categoria\", \"topWord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a66ec06e-60e4-451e-a944-965ebc671fd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>categoria</th><th>topWord</th></tr></thead><tbody><tr><td>finance</td><td>List(series, also, company, masters, new, one, universe, first, heman, voiced)</td></tr><tr><td>medicine</td><td>List(hospital, health, medical, new, university, also, medicine, research, first, school)</td></tr><tr><td>research</td><td>List(research, university, medical, health, also, medicine, institute, national, new, professor)</td></tr><tr><td>technology</td><td>List(game, also, software, linux, new, released, version, system, first, one)</td></tr><tr><td>energy</td><td>List(power, station, plant, energy, company, electricity, mw, gas, also, electric)</td></tr><tr><td>transport</td><td>List(station, bridge, line, new, airport, also, railway, two, opened, terminal)</td></tr><tr><td>politics</td><td>List(one, party, nation, hanson, election, latham, australian, australia, new, queensland)</td></tr><tr><td>culture</td><td>List(film, meitei, language, also, manipur, khamba, manipuri, dance, one, indian)</td></tr><tr><td>science</td><td>List(aircraft, air, force, lockheed, first, flight, also, two, us, one)</td></tr><tr><td>humanities</td><td>List(university, also, later, first, new, film, one, archaeology, history, work)</td></tr><tr><td>economics</td><td>List(party, university, socialist, new, also, international, workers, labour, economic, economics)</td></tr><tr><td>trade</td><td>List(park, national, glacier, river, also, new, mountain, fort, lake, company)</td></tr><tr><td>sports</td><td>List(open, first, tennis, doubles, singles, won, us, round, final, title)</td></tr><tr><td>pets</td><td>List(species, also, dog, greyhound, stadium, breed, first, cat, references, dogs)</td></tr><tr><td>engineering</td><td>List(also, river, water, first, dresden, one, new, used, german, bridge)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "finance",
         [
          "series",
          "also",
          "company",
          "masters",
          "new",
          "one",
          "universe",
          "first",
          "heman",
          "voiced"
         ]
        ],
        [
         "medicine",
         [
          "hospital",
          "health",
          "medical",
          "new",
          "university",
          "also",
          "medicine",
          "research",
          "first",
          "school"
         ]
        ],
        [
         "research",
         [
          "research",
          "university",
          "medical",
          "health",
          "also",
          "medicine",
          "institute",
          "national",
          "new",
          "professor"
         ]
        ],
        [
         "technology",
         [
          "game",
          "also",
          "software",
          "linux",
          "new",
          "released",
          "version",
          "system",
          "first",
          "one"
         ]
        ],
        [
         "energy",
         [
          "power",
          "station",
          "plant",
          "energy",
          "company",
          "electricity",
          "mw",
          "gas",
          "also",
          "electric"
         ]
        ],
        [
         "transport",
         [
          "station",
          "bridge",
          "line",
          "new",
          "airport",
          "also",
          "railway",
          "two",
          "opened",
          "terminal"
         ]
        ],
        [
         "politics",
         [
          "one",
          "party",
          "nation",
          "hanson",
          "election",
          "latham",
          "australian",
          "australia",
          "new",
          "queensland"
         ]
        ],
        [
         "culture",
         [
          "film",
          "meitei",
          "language",
          "also",
          "manipur",
          "khamba",
          "manipuri",
          "dance",
          "one",
          "indian"
         ]
        ],
        [
         "science",
         [
          "aircraft",
          "air",
          "force",
          "lockheed",
          "first",
          "flight",
          "also",
          "two",
          "us",
          "one"
         ]
        ],
        [
         "humanities",
         [
          "university",
          "also",
          "later",
          "first",
          "new",
          "film",
          "one",
          "archaeology",
          "history",
          "work"
         ]
        ],
        [
         "economics",
         [
          "party",
          "university",
          "socialist",
          "new",
          "also",
          "international",
          "workers",
          "labour",
          "economic",
          "economics"
         ]
        ],
        [
         "trade",
         [
          "park",
          "national",
          "glacier",
          "river",
          "also",
          "new",
          "mountain",
          "fort",
          "lake",
          "company"
         ]
        ],
        [
         "sports",
         [
          "open",
          "first",
          "tennis",
          "doubles",
          "singles",
          "won",
          "us",
          "round",
          "final",
          "title"
         ]
        ],
        [
         "pets",
         [
          "species",
          "also",
          "dog",
          "greyhound",
          "stadium",
          "breed",
          "first",
          "cat",
          "references",
          "dogs"
         ]
        ],
        [
         "engineering",
         [
          "also",
          "river",
          "water",
          "first",
          "dresden",
          "one",
          "new",
          "used",
          "german",
          "bridge"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "categoria",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "topWord",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41549bed-e90f-4070-b571-3e7d15a416c4",
     "showTitle": true,
     "title": "Costruzione del modello"
    }
   },
   "source": [
    "In questa parte ci occupiamo di addestrare e testare un modello per la classificazione del testo a partire dalla colonna \"documents\". Innazitutto creiamo una colonna \"label\" numerica a partire dalla colonna \"categoria\" testuale e dividiamo il dataset in 80% di train e 20% di test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "face3344-fbac-48b1-b6e9-eba38cf671ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"categoria\", outputCol=\"label\")\n",
    "\n",
    "df_2 = indexer.fit(df).transform(df)\n",
    "train_data, test_data = df_2.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a5d9e8-4a79-47ea-bd51-d64ca1d2fef5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A questo punto, creiamo una pipeline divisa nelle seguenti fasi:\n",
    "1) tokenizzazione del testo; \n",
    "2) rimozione delle stopwors; \n",
    "3) conteggio delle parole presenti a partire da un vocabolario di 1000 parole; \n",
    "4) addestramento di una rete neurale fully connected con 1000 neuroni di input (tanti quanti la dimensione del vocabolario), due hidden layer di ampiezza 64 e 32 e un output layer di 15 neuroni visto che abbiamo 15 classi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8387e963-4209-454b-9928-d8025432f6c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"documents\", outputCol=\"words\")\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"features\", vocabSize = 1000)\n",
    "\n",
    "nn =  MultilayerPerceptronClassifier(layers=[1000, 64, 32, 15], maxIter=100, solver='l-bfgs', tol=0.005, seed=1234)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer, \n",
    "    remover, \n",
    "    cv,  \n",
    "    nn\n",
    "])\n",
    "\n",
    "\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "835f053c-b286-4134-b737-b2296323b2c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Valutiamo quindi il modello sull'insieme di test, essendo bilanciato il dataset (come visto al punto 1) possiamo usare la accuracy come metrica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08bee21b-b318-4844-a321-e2bf64db7b35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy =  0.9087973640856672\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "predictions = model.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2690f27d-b925-41a5-93f7-24aff4d32e39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Proviamo a vedere se cambia qualcosa addestrando il modello sulla colonna \"summary\" al posto della colonna \"documents\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf392554-e584-4a09-99d4-7e715f5dcf80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"summary\", outputCol=\"words\")\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"features\", vocabSize = 1000)\n",
    "\n",
    "nn =  MultilayerPerceptronClassifier(layers=[1000, 64, 32, 15], maxIter=100, solver='l-bfgs', tol=0.005, seed=1234)\n",
    "\n",
    "pipeline_summary = Pipeline(stages=[\n",
    "    tokenizer, \n",
    "    remover, \n",
    "    cv,  \n",
    "    nn\n",
    "])\n",
    "\n",
    "\n",
    "model_summary = pipeline_summary.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a07893f7-3e0d-419a-a7f9-978308b06b4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy =  0.8658978583196046\n"
     ]
    }
   ],
   "source": [
    "predictions_summary = model_summary.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions_summary)\n",
    "print(\"Test Accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caf2b957-1620-4738-a3f4-8b21e9a8801c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Si può osservare che lo stesso modello addestrato sulla colonna \"documents\" ha una accuracy maggiore rispetto a quella ottenuta nella cella precedente."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Wikipedia project",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
